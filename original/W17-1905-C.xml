<?xml version="1.0" encoding="UTF-8"?>
<doc id="W17-1905" editor="C" format="WS" position="S" region="NN">
    <title>
        <text>Classifying Lexical-semantic Relationships by Exploiting Sense/Concept Representations</text>
    </title>
    <abstract>
        <text>This paper proposes a method for classifying the type of lexical-semantic relation between a given pair of words. Given an inventory of target relationships, this task can be seen as a multi-class classification problem. We train a supervised classifier by assuming that a specific type of lexical-semantic relation between a pair of words would be signaled by a carefully designed set of relation-specific similarities between the words. These similarities are computed by exploiting “sense representations” (sense/concept embeddings). The experimental results show that the proposed method clearly outperforms an existing state-of-the-art method that does not utilize sense/concept embeddings, thereby demonstrating the effectiveness of the sense representations.</text>
    </abstract>   
    <introduction>
        <text>Given a pair of words, classifying the type of lexical-semantic relation that could hold between them may have a range of applications. In particular, discovering typed lexical-semantic relation instances is vital in building a new lexical-semantic resource, as well as for populating an existing lexical-semantic resource. As argued in (BoydGraber et al., 2006), even Princeton WordNet (henceforth PWN) (Miller, 1995) is noted for its sparsity of useful internal lexical-semantic relations. A distributional thesaurus (Weeds et al., 2014), usually built with an automatic method such as that described in (Rychly and Kilgarriff, 2007), often comprises a disorganized semantic network internally, where a variety of lexical-semantic relations are incorporated without having proper relation labels attached. These issues could be addressed if an accurate method for classifying the type of lexical-semantic relation is available.</text>
        <text>\n\n A number of research studies on the classification of lexical-semantic relationships have been conducted. Among them, Necsulescu et al. (2015) recently presented two classification methods that utilize word-level feature representations including word- embedding vectors. Although the reported results are superior to the compared systems, neither of the proposed methods exploited “sense representations,” which are described as the fine-grained representations of word senses, concepts, and entities in the description of this workshop.</text>
        <text>\n\n Motivated by the above-described issues and previous work, this paper proposes a supervised classification method that exploits sense representations, and discusses their utilities in the lexical relation classification task. The major rationales behind the proposed method are</text>
        <edit type="clarity;style" crr="as follows" comments="Consider inserting to amplify meaning"></edit>
        <text>: (1) a specific type of lexical-semantic relation between a pair of words would be indicated by a</text>
        <edit type="punctuation;hyphenation" crr="carefully-designed" comments="">carefully designed</edit>
        <text>set of relation-specific similarities associated with the words; and (2) the similarities could be effectively computed by exploiting sense representations.</text>
        <text>\n\n More specifically, for each word in the pair, we first collect relevant sets of sense/concept nodes (node sets) from an existing lexical-semantic resource (PWN), and then compute similarities for some designated pairs of node sets, where each node is represented by an embedding vector depending on its type (sense/concept). In terms of its design, each node set pair is constructed such that it is associated with a specific type of lexical-semantic relation. The resulting array of similarities, along with the underlying word/sense/concept embedding vectors, is finally fed into the classifier as features.</text>
        <text>\n\n The empirical results that use the BLESS dataset (Baroni and Lenci, 2011) demonstrate that our method clearly outperformed existing state-of-the-art methods (Necsulescu et al., 2015) that did not employ sense/concept embeddings</text>
        <edit type="readability" crr=". This confirms" comments="Style / long sentence. Consider splitting.">, confirming</edit>
        <text>that properly combining the similarity features also with the underlying semantic/conceptual-level embeddings is indeed effective. These results, in turn, highlight the utility of “the sense representations” (the sense/concept embeddings) created by the existing system referred to as AutoExtend (Rothe and Schutze, 2015).</text>
        <text>\n\n The remainder of the paper first reviews related work (section 2), and then presents our approach (section 3). As our experiments (section 4) utilize the BLESS dataset, the experimental results are directly compared with that of (Necsulescu et al., 2015) (section 5). Although our methods were proved to be superior through the experiments, our operational requirement (sense/concept embeddings should be created from the underlying lexical-semantic resource) could be problematic</text>
        <edit type="readability" crr="," comments="Punctuation / readability / long sentence"></edit>
        <text>especially when having to process unknown words. We conclude the present paper by discussing future work</text>
        <edit type="clarity" crr="required" comments=""></edit>
        <text>to address this issue (section 6).</text>
    </introduction>   
</doc>