<?xml version="1.0" encoding="UTF-8"?>
<doc id="P15-1151" editor="A" format="Conf" position="S" region="NN">
    <title>
        <text>A Convolutional Architecture for Word Sequence Prediction</text>
    </title>
    <abstract>
        <text>We propose a convolutional neural network, named genCNN, for word sequence prediction. Different from previous work on neural network-based language modeling and generation (e.g., RNN or LSTM), we choose not to greedily summarize the history of words as a</text>
        <edit type="hyphenation" crr="fixed-length" comments="hyphenation - hyphen needed in compound adjective">fixed length</edit>
        <text>vector. Instead, we use a convolutional neural network to predict the next word with</text>
        <edit type="readability;clarity" crr="a variable-length history of words." comments="readability/clarity - original was awkward but also ambiguous: is the history variable length or the words? This clarifies it.">the history of words of variable length.</edit>
        <text>Also different from the existing feedforward networks for language modeling, our model can effectively fuse the local correlation and global correlation in the word sequence</text>
        <edit type="punctuation" crr="" comments="punctuation - comma is inappropriate here">,</edit>
        <text>with a convolution-gating strategy specifically designed for the task. We argue that our model can give adequate representation of the history, and therefore can naturally exploit both the</text>
        <edit type="hyphenation" crr="short- and long-range dependencies." comments="hyphenation - need to hypheante the compound adjective including with suspended hyphen on the first term">short and long range dependencies.</edit>
        <text>Our model is fast, easy to train, and readily parallelized. Our extensive experiments on text generation and n-best re-ranking in machine translation show that genCNN outperforms the</text>
        <edit type="word choice" crr="state-of-the-art models" comments="word choice - plural on state-of-the-art seems awkward. I think 'models' is the appropriate choice, but I might change my mind after reading the full paper">state-of-the-arts</edit>
        <text>with big margins.</text>
    </abstract>   
    <introduction>
        <text>Both language modeling (Wu and Khudanpur, 2003; Mikolov et al., 2010; Bengio et al., 2003) and text generation (Axelrod et al., 2011) boil down to modeling the conditional probability of a word given the</text>
        <edit type="word choice" crr="preceding" comments="word choice - preceding is what comes before, proceeding is (among other uses) a series of steps to do something">proceeding</edit>
        <text>words. Previously,</text>
        <edit type="clarity" crr="this modeling" comments="clarity - it's not clear what 'it' refers back to">it</edit>
        <edit type="tense" crr="has mostly been done" comments="tense - with 'previousliy' we want past tense">is mostly done</edit>
        <text>through purely memory-based approaches, such as n-grams, which cannot deal with long sequences and</text>
        <edit type="grammar" crr="have" comments="number agreement - 'have' to agree with 'approaches'">has</edit>
        <text>to use</text>
        <edit type="redundancy" crr="" comments="conciseness / redundancy - some is redundant here as heuristics is plural, so we already know there's a 'some' implied">some</edit>
        <text>heuristics (called smoothing) for rare ones. Another family of methods</text>
        <edit type="grammar" crr="is" comments="number agreement - 'is' to agree with 'family'">are</edit>
        <text>based on distributed representations of words </text>
        <edit type="clarity" crr="and is" comments="clarity - the original read like the 'which' clause was tied to representations, where it should be tied to the family of methods">, which is</edit>
        <text>usually tied with a neural network (NN) architecture for estimating the conditional probabilities of words.</text>
        <text>\\ Two categories of neural networks have been used for language modeling: 1) recurrent neural networks (RNN) and 2) feedfoward networks (FFN):</text>
        <text>\ The RNN-based models, including its variants like LSTM, enjoy more popularity, mainly due to their flexible structures for processing word sequences of arbitrary lengths and their recent empirical success (Sutskever et al., 2014; Graves, 2013). We</text>
        <edit type="flow" crr="argue, however, that" comments="this reads better">however argue that</edit>
        <text>RNNs, with their power built on the recursive use of relatively simple computation units, are forced to make greedy summarization of the history and consequently</text>
        <edit type="readability" crr="are" comments="readability - this clause could be tied to the previous 'are' grammatically, but it doesn't read well"></edit>
        <text>not efficient on modeling word sequences, which clearly have a bottom-up structures.</text>
        <text>\ The FFN-based models, on the other hand, avoid this difficulty by feeding directly on the history. However, the FFNs are built on fully connected networks, rendering them inefficient on capturing local structures of languages. Moreover their rigid architectures make it futile to handle the great variety of patterns in long- range correlations of words.</text>
        <text>\\ We propose a novel convolutional architecture, named genCNN, as a model that can efficiently combine</text>
        <edit type="hyphenation" crr="local- and long-range" comments="hyphenation - hyphen, including suspended hyphen on first term, is needed in this compound adjective">local and long range</edit>
        <text>structures of language for the purpose of modeling conditional probabilities. genCNN can be directly used in generating a word sequence (i.e., text generation) or evaluating the likelihood of word sequences (i.e., language modeling). We also show the empirical superiority of genCNN on both tasks over traditional n-grams and its RNN or FFN counterparts.</text>
    </introduction>   
</doc>