<?xml version="1.0" encoding="UTF-8"?>
<doc id="P11-1148" editor="A" format="Conf" position="NS" region="N">
    <title>
        <text>Latent Semantic Word Sense Induction and Disambiguation</text>
    </title>
    <abstract>
        <text>In this paper, we present a unified model for the automatic induction of word senses from text, and the subsequent disambiguation of particular word instances using the automatically extracted sense inventory. The</text>
        <edit type="conciseness" crr="induction and disambiguation steps" comments="conciseness - just tightening it up a little bit.">induction step and the disambiguation step</edit>
        <text>are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions</text>
        <edit type="readability" crr=". In" comments="readability - this sentence is getting a bit long, so splitting it in two here.">; in</edit>
        <text>a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions. The model is evaluated on the SEMEVAL-2010 word sense induction and disambiguation</text>
        <edit type="clarity;accuracy" crr="tasks" comments="clarity / accuracy - based on my reading of the SemEval 2010 page, these are two separate tasks, although the remainder of the paper may clarify this further.">task</edit>
        <text>, on which it reaches state-of-the-art results.</text>
    </abstract>   
    <introduction>
        <text>Word sense induction (WSI) is the task of automatically identifying the senses of words in texts, without the need for handcrafted resources or manually annotated data. The manual construction of a sense inventory is a tedious and time-consuming job, and the result is highly dependent on the annotators and the domain at hand. By applying an automatic procedure, we are able to only extract the senses that are objectively present in a particular corpus, and it allows for the sense inventory to be straightforwardly adapted to a new domain.</text>
        <text>\n\n Word sense disambiguation (WSD), on the other hand, is the closely related task of assigning a sense label to a particular instance of a word in context</text>
        <edit type="punctuation" crr="" comments="punctuation - comma is not appropriate.">,</edit>
        <text>using an existing sense inventory. The bulk of WSD algorithms up till now use</text>
        <edit type="hyphenation" crr="predefined" comments="hyphenation - pre- usually doesn't take a hyphen unless the resulting word is awkward or confusing. This can be style specific.">pre-defined</edit>
        <text>sense inventories (such as WordNet) that often contain fine-grained sense distinctions, which poses serious problems for computational semantic processing (Ide and Wilks, 2007). Moreover, most WSD algorithms take a supervised approach, which requires a significant amount of manually annotated training data</text>
        <edit type="punctuation" crr="." comments="punctuation - comma not needed here"></edit>
        <text>\n\n The model presented here induces the senses of words in a fully unsupervised way</text>
        <edit type="punctuation" crr="" comments="punctuation - comma not needed here">,</edit>
        <text>and subsequently uses the induced sense inventory for the unsupervised disambiguation of particular occurrences of words. The</text>
        <edit type="conciseness" crr="induction and disambiguation steps" comments="conciseness, same change as made in abstract">induction step and the disambiguation step</edit>
        <text>are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The key idea is that the model combines tight, </text>
        <edit type="hyphenation" crr="synonym-like" comments="hyphenation - -like takes hyphen unless the word is listed in closed form in the dictionary. This can be style specific.">synonym like</edit>
        <text>similarity (based on dependency relations) with broad, topical similarity (based on a large</text>
        <edit type="punctuation" crr="“bag of words”" comments="punctuation - double quotes around phrases like this - but this can be style specific. In particular, Australian English uses single quotes.">‘bag of words’</edit>
        <text>context window). The intuition in this is that the dependency features can be disambiguated by the topical dimensions identified by the broad contextual features</text>
        <edit type="readability" crr=". In" comments="readability - this sentence is getting a bit long, so splitting it in two here.">; in</edit>
        <text>a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions (based on the instance’s context words).</text>
        <text>\n\n The paper is organized as follows. Section 2 presents some previous research on distributional similarity and word sense induction. Section 3 gives an overview of our method for word sense induction and disambiguation. Section 4 provides a quantitative evaluation and comparison to other algorithms in the framework of the SEMEVAL-2010 word sense induction and disambiguation (WSI/WSD)</text>
        <edit type="clarity;accuracy" crr="tasks" comments="clarity / accuracy - based on my reading of the SemEval 2010 page, these are two separate tasks, although the remainder of the paper may clarify this further.">task</edit>
        <text>. The last section draws conclusions</text>
        <text>and lays out a number of future research directions.</text>
    </introduction>   
</doc>