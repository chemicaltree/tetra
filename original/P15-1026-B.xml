<?xml version="1.0" encoding="UTF-8"?>
<doc id="P15-1026" editor="B" format="Conf" position="S" region="NN">
    <title>
        <text>Question Answering over Freebase with Multi-Column Convolutional Neural Networks</text>
    </title>
    <abstract>
        <text>Answering natural language questions</text>
        <edit type="word choice" crr="in" comments="">over</edit>
        <text>a knowledge base is an important and challenging task. Most</text>
        <edit type="grammar" crr="" comments="no preposition needed here">of</edit>
        <text>existing systems typically rely on hand-crafted features and rules to conduct question understanding and/or answer ranking. In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to understand questions from three different aspects (namely, answer path, answer context, and answer type) and learn their distributed representations. Meanwhile, we jointly learn low-dimensional embeddings of entities and relations in the knowledge base. Question-answer pairs are used to train the model to rank candidate answers. We also leverage question paraphrases to train the column networks in a multi-task learning</text>
        <edit type="word choice;style" crr="framework" comments="word choice; manner seems like an odd choice here.">manner</edit>
        <text>. We use FREEBASE as the knowledge base and conduct extensive experiments on the WEBQUESTIONS dataset. Experimental results show that our method achieves</text>
        <edit type="word order" crr="comparable or better" comments="">better or comparable</edit>
        <text>performance compared with baseline systems. In addition, we develop a method to compute the salience scores of question words in different column networks. The results help us intuitively understand what MCCNNs learn.</text>
    </abstract>   
    <introduction>
        <text>Automatic</text>
        <edit type="hyphenation" crr="question-answering" comments="creating a combined adjective">question answering</edit>
        <text> systems return direct and exact answers to natural language questions. In recent years, the development of large-scale knowledge bases, such as FREEBASE (Bollacker et al., 2008),</text>
        <edit type="grammar" crr="have provide" comments="">provides</edit>
        <text>a rich resource</text>
        <edit type="word choice" crr="for answering" comments="">to answer</edit>
        <text>open-domain questions. However,</text>
        <edit type="conciseness" crr="understand" comments="">how to understand</edit>
        <text>questions and bridging he gap between natural languages and the structured semantics of knowledge bases is still very challenging.</text>
        <text>\\ Up</text>
        <edit type="word choice" crr="until" comments="">to</edit>
        <text>there have been two mainstream methods for</text>
        <edit type="clarity" crr="handling" comments="added for clarity"></edit>
        <text>this task. The first</text>
        <edit type="conciseness" crr="" comments="">one</edit>
        <text>is based on semantic parsing (Berant et al., 2013; Berant and Liang, 2014)</text>
        <edit type="readability" crr="while" comments="word choice; discretionary">and</edit>
        <edit type="clarity" crr="the second" comments="">the other</edit>
        <text>relies on information extraction over the structured knowledge base (Yao and Van Durme, 2014; Bordes et al., 2014a; Bordes et al., 2014b).</text>
        <edit type="conciseness" crr="Semantic" comments="extraneous">The semantic</edit>
        <text>parsers learn to understand natural language questions by converting them into logical forms</text>
        <edit type="readability" crr=", then" comments="combining sentences">. Then,</edit>
        <text>the parse results are used to generate structured queries to search knowledge bases and obtain the answers. Recent works mainly focus on using question-answer pairs, instead of annotated logical forms of questions, as weak training signals (Liang et al., 2011; Krishnamurthy and Mitchell, 2012) to reduce annotation costs. However, some of</text>
        <edit type="word choice" crr="these" comments="">them</edit>
        <text>still assume a fixed and pre-defined set of lexical triggers which limit their domains and scalability</text>
        <edit type="conciseness" crr="." comments="">capability.</edit>
        <text>In addition, they need to manually design features for semantic parsers. The second approach uses information- extraction techniques for open question answering. These methods retrieve a set of candidate answers from the knowledge base,</text>
        <edit type="readability" crr="then" comments="I'm assuming this is a series of steps.">and the</edit>
        <text>extract features for the question and these candidates to rank them. However, the method proposed by Yao and Van Durme (2014) relies on rules and dependency parse results to extract hand-crafted features for questions. Moreover, some methods (Bordes et al., 2014a; Bordes et al., 2014b) use the summation of question word embeddings to represent questions, which ignores word order information and cannot process complicated questions.</text>
        <text>\\ In this paper, we introduce multi-column convolutional neural networks (MCCNNs) to automatically analyze</text>
        <edit type="word order" crr="multiple aspects of questions." comments="">questions from multiple aspects.</edit>
        <text>Specifically, the model shares the same word embeddings</text>
        <edit type="word choice" crr="for representing" comments="">to represent</edit>
        <text>question words. MCCNNs use different column networks to extract answer types, relations, and context information from the input questions. The entities and relations in the knowledge base</text>
        <edit type="style;punctuation" crr="(i.e., FREEBASE in our experiments)" comments="">(namely FREEBASE in our experiments)</edit>
        <text>are also represented as low-dimensional vectors. Then, a score layer is employed to rank candidate answers according to the representations of questions and candidate answers. The proposed information-extraction-based method utilizes question-answer pairs to automatically learn the model without relying on manually annotated logical forms and hand-crafted features.</text>
        <edit type="flow;word choice" crr="In addition, we do not rely on pre-defined lexical triggers and rules. The question" comments="">We also do not use any pre-defined lexical triggers and rules. In addition, the question</edit>
        <text>paraphrases are also used to train networks and generalize for the unseen words in a multi-task learning manner. We have conducted extensive experiments on WEBQUESTIONS. Experimental results</text>
        <edit type="word choice" crr="show" comments="">illustrate</edit>
        <text>that our method outperforms several baseline systems.</text>
        <text>\\ The contributions of this paper are three-fold: \ We introduce multi-column convolutional neural networks</text>
        <edit type="clarity" crr="(MCCNNs)" comments="added for clarity"></edit>
        <text>for question understanding without relying on hand-crafted features and rules, and use question paraphrases to train the column networks and word vectors in a multi-task learning</text>
        <edit type="word choice;style" crr="framework;" comments="word choice; manner seems like an odd choice here.">manner;</edit>
        <text>\ We</text>
        <edit type="style;word choice" crr="simultaneously" comments="">jointly</edit>
        <text>learn low-dimensional embeddings for the entities and relations in FREEBASE with question-answer pairs as supervision signals; \ We conduct extensive experiments</text>
        <edit type="word choice" crr="using" comments="">on</edit>
        <text>the WEBQUESTIONS dataset, and provide some intuitive interpretations for MCCNNs by developing a method to detect salient question words in the different column networks.</text>
    </introduction>   
</doc>