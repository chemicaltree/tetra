<?xml version="1.0" encoding="UTF-8"?>
<doc id="W14-4012" editor="B" format="WS" position="NS" region="N">
    <title>
        <text>On the Properties of Neural Machine Translation: Encoder–Decoder Approaches</text>
    </title>
    <abstract>
        <text>Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models:; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns</text>
        <edit type="grammar" crr="the" comments="">a</edit>
        <text>grammatical structure of a sentence automatically.</text>
    </abstract>   
    <introduction>
        <text>A new approach for statistical machine translation based purely on neural networks has recently been proposed (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014). This new approach, which we refer to as neural machine translation, is inspired by the recent trend of deep representational learning. All</text>
        <edit type="clarity" crr="of" comments="added for clarity; this may be somewhat discretionary"></edit>
        <text>the neural network models used in (Sutskever et al., 2014; Cho et al., 2014) consist of an encoder and a decoder. The encoder extracts a fixed-length vector representation from a variable-length input sentence, and from this representation, the decoder generates a correct, variable-length target translation.</text>
        <text>\n\n The emergence of the neural machine translation is highly significant, both practically and theoretically. Neural machine translation models require only a fraction of the memory needed by traditional statistical machine translation (SMT) models. The models we trained for this paper require only 500MB of memory in total. This stands in stark contrast</text>
        <edit type="grammar" crr="to" comments="">with</edit>
        <text>existing SMT systems, which often require tens of gigabytes of memory. This makes the neural machine translation appealing in practice. Furthermore, unlike conventional translation systems, each and every component of the neural translation model is trained jointly to maximize the translation performance.</text>
        <text>\n\n As this approach is relatively new,</text>
        <edit type="word order" crr="not much work has been done in relation to" comments="">there has not been much work on</edit>
        <text>analyzing the properties and behavior of these models. For instance</text>
        <edit type="clarity" crr=", some further questions about this approach that are worth considering include the following" comments=""></edit>
        <text>: What are the properties of sentences on which this approach performs better? How does the choice of source/target vocabulary affect the performance? In</text>
        <edit type="word choice" crr="what" comments="word choice; in general, you use what when you're dealing with an unknown number and which when you're dealing with a choice between a known set of options">which</edit>
        <text>cases does the neural machine translation fail?</text>
        <text>\n\n It is crucial to understand the properties and behavior of this new neural machine translation approach in order to determine future research directions. Also, understanding the weaknesses and strengths of neural machine translation might lead to better ways of integrating SMT and neural machine translation systems.</text>
        <text>In this paper, we analyze two neural machine translation models. One of them is the RNN Encoder–Decoder</text>
        <edit type="conciseness" crr="" comments="">that was</edit>
        <text>proposed recently in (Cho et al., 2014). The other model replaces the encoder in the RNN Encoder–Decoder model with a novel neural network, which we call a gated recursive convolutional neural network (grConv). We evaluate these two models on the task of translation from French to English.</text>
        <text>\n\n Our analysis shows that the performance of the neural machine translation model degrades quickly as the length of a source sentence increases. Furthermore, we find that the vocabulary size has a</text>
        <edit type="word choice" crr="large" comments="">high</edit>
        <text>impact on the translation performance. Nonetheless, qualitatively</text>
        <edit type="punctuation" crr="," comments=""></edit>
        <text>we find that</text>
        <edit type="grammar" crr="" comments="">the</edit>
        <text>both models are able to generate correct translations most of the time.</text>
        <edit type="word choice" crr="\n\n Finally" comments="">\n\n Furthermore</edit> 
        <text>, the newly proposed grConv model is able to learn, without supervision, a kind of syntactic structure over the source language.</text>
    </introduction>   
</doc>