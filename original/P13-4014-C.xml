<?xml version="1.0" encoding="UTF-8"?>
<doc id="P13-4014" editor="C" format="Conf" position="NS" region="N">
    <title>
        <edit type="capitalization" crr="QuEst - A Translation-quality Estimation Framework" comments="Capitalize titles;Punctuation / hyphenate when qualifying a noun that follows">QuEst - A translation quality estimation framework</edit>
    </title>
    <abstract>
        <text>We describe QuEst, an open source framework for machine translation quality estimation. The framework allows the extraction of several quality indicators from source segments, their translations, external resources (corpora, language models, topic models, etc.), as well as language tools (parsers, part-of-speech tags, etc.). It also provides machine learning algorithms to build quality estimation models. We benchmark the framework on a number of datasets and discuss the efficacy of</text>
        <edit type="clarity" crr="several" comments="Word usage / insert to qualify a little"></edit>
        <text>features and algorithms.</text>
    </abstract>   
    <introduction>
        <text>As</text>
        <edit type="capitalization;consistency" crr="machine translation" comments="Capitalization not required">Machine Translation</edit>
        <text>(MT) systems become widely adopted</text>
        <edit type="word choice" crr="for both the purposes of gisting and for producing" comments="">both for gisting purposes and to produce</edit>
        <text>professional quality translations, automatic methods are needed for predicting the quality of a translated segment. This is referred to as</text>
        <edit type="capitalization;consistency;punctuation" crr="“quality estimation”" comments="Capitalization not required; Punctuation / better use inverted commas when referring to a quote or term">Quality Estimation</edit>
        <text>(QE). Different from standard MT evaluation metrics, QE metrics do not have access to reference (human) translations</text>
        <edit type="punctuation" crr=". Instead," comments="Punctuation / ending sentence is better in this case">;</edit>
        <text>they are aimed at MT systems in use. QE has a number of applications, including:</text>
        <edit type="capitalization;consistency" crr="\ deciding which segments need revision by a translator (quality assurance);" comments="Capitalization / as this is not a complete sentence but a bullet, use lower case">\ Deciding which segments need revision by a translator (quality assurance);</edit>
        <edit type="capitalization;consistency;style;word choice" crr="\ assessing whether a reader obtains a reliable understanding of the text;" comments="alternative is more appropriate; Style / alternative is better for academic writing;Capitalization / as this is not a complete sentence but a bullet, use lower case">\ Deciding whether a reader gets a reliable gist of the text;</edit>
        <edit type="capitalization;consistency" crr="\ selecting among alternative translations produced by different MT systems;" comments="Capitalization / as this is not a complete sentence but a bullet, use lower case">\ Selecting among alternative translations produced by different MT systems;</edit>
        <edit type="capitalization;consistency;word choice" crr="\ determining whether the translation can be used for self-training of MT systems." comments="Capitalization / as this is not a complete sentence but a bullet, use lower case">\ Deciding whether the translation can be used for self-training of MT systems.</edit>
        <edit type="flow" crr="\\ Inspired by the confidence scores used in speech recognition which was mostly due to the estimation of word posterior probabilities work in QE for MT started in the early 2000's." comments="Meaning and flow / sentence re-structure required">\\ Work in QE for MT started in the early 2000's, Iinspired by the confidence scores used in Speech Recognition: mostly the estimation of word posterior probabilities.</edit>
        <text>Back then it was called confidence estimation, which we believe is a narrower term.</text>
        <edit type="readability;clarity" crr="In 2003, a six-week workshop was held on the topic at John Hopkins University (Blatz et al., 2004) with the key goal being to estimate automatic" comments="Sentence structure / invert / try to put time first (a good rule is time before manner before place)">A six-week workshop on the topic at John Hopkins University in 2003 (Blatz et al., 2004) had as goal to estimate automatic</edit>
        <text>metrics such as BLEU (Papineni et al., 2002) and WER. These metrics are difficult to interpret, particularly at the sentence-level, and results of their</text>
        <edit type="style" crr="numerous" comments="Meaning / alternative is better for academic writing">very many</edit>
        <text>trials proved unsuccessful. The overall quality of MT was considerably lower at the time, and therefore pinpointing the very few good quality segments was a</text>
        <edit type="style" crr="difficult" comments="">hard</edit>
        <edit type="clarity" crr="to solve" comments=""></edit>
        <edit type="word choice" crr=". Neither" comments="Vocabulary / use the 'neither / nor' rule when applying a double negative">. No</edit>
        <text>software nor datasets were made available after the workshop.</text>
        <edit type="word choice" crr="\\ There has been a new surge" comments="Meaning / sentence restructure required">\\ A new surge</edit>
        <text>of interest in the field</text>
        <edit type="redundancy" crr="" comments="not required">started</edit>
        <text>recently, motivated by the widespread used of MT systems in the translation industry, as a consequence of</text>
        <edit type="style" crr="improved" comments="Word usage / 'better' is correct, but alternative better for academic writing">better</edit>
        <text>translation quality, more user-friendly tools, and higher demand for translation. In order to make MT maximally useful in this scenario, a quantification of the quality of translated segments</text>
        <edit type="clarity" crr="that is" comments="Clarify / insert to clarify"></edit>
        <text>similar to “fuzzy match scores” from translation memory systems is needed. QE work addresses this problem by using more complex metrics that go beyond matching the source segment with previously translated data. QE can also be useful for end-users reading translations for gisting, particularly those who cannot read the source language.</text>
        <edit type="readability" crr="\\ Today, QE" comments="Insert using rule time before manner before place">\\ QE nowadays</edit>
        <text>focuses on estimating more interpretable metrics. “Quality” is defined according to the application: post-editing, gisting, etc. A number of positive results have been reported. Examples include improving post-editing efficiency by filtering out low-quality segments which would require more effort or time to correct than translating from scratch (Specia et al., 2009; Specia, 2011), selecting high-quality segments to be published as they are, without post-editing (Soricut and Echihabi, 2010), selecting a translation from either an MT system or a translation memory for post-editing (He et al., 2010), selecting the best translation from multiple MT systems (Specia et al., 2010), and highlighting sub-segments that need revision (Bach et al., 2011).</text>
        <text>\\ QE is generally addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of translations described through a number of features and annotated for quality. For an overview of various algorithms and features we refer the reader to the WMT12 shared task on QE (Callison-Burch et al., 2012). Most of the research work lies on deciding which aspects of quality are more relevant for a given task and designing feature extractors for them. While simple features such as counts of tokens and language model scores can be easily extracted, feature engineering for more advanced and useful information can be quite labour-intensive. Different language pairs or optimi`sation against specific quality scores (e.g., post-editing time vs translation adequacy) can benefit from very different feature sets.</text>
        <text>\\ QuEst, our framework for quality estimation, provides a wide range of feature extractors from source and translation texts</text>
        <edit type="repetitiveness;word choice" crr=", as well as" comments="Word usage / repetition of and / replace">and</edit>
        <text>external resources and tools (Section 2). These go from simple, language-independent features, to advanced, linguistically-motivated features. They include features that rely on information from the MT system that generated the translations, and features that are oblivious to the way translations were produced (Section 2.1). In addition, by integrating a well-known machine learning toolkit</text>
        <edit type="word choice;conciseness" crr="called" comments="Word usage / punctuation / insert this to clarify and reduce commas">,</edit>
        <text>scikit-learn</text>
        <edit type="word choice;readability" crr="with" comments="Conjunction / with is more appropriate in this context, given your use of the verb 'combine'">, and</edit>
        <text>algorithms that are known to perform well on this task, QuEst provides a simple and effective way of experimenting with techniques for feature selection and model building, as well as parameter optimisation through grid search (Section 2.2). In Section 3, we present experiments using the framework with nine QE datasets.</text>
        <text>\\ In addition to providing a practical platform for quality estimation, by freeing researchers from feature engineering, QuEst will facilitate work on the learning aspect of the problem. Quality estimation poses several machine learning challenges, such as the fact that it can exploit a large, diverse, but often noisy set of information sources, with a relatively small number of annotated data points</text>
        <edit type="readability" crr=". It" comments="">, and</edit>
        <text>relies on human annotations that are often inconsistent due to the subjectivity of the task (quality judgements). Moreover, QE is highly non-linear: unlike many other problems in language processing, considerable improvements can be achieved using non-linear kernel techniques. Also, different applications for the quality predictions may benefit from different machine learning techniques, an aspect that has been mostly neglected so far. Finally, the framework will also facilitate research on ways of using quality predictions in novel extrinsic tasks, such as self-training of statistical machine translation systems, and for estimating quality in other text output applications such as text</text>
        <edit type="spelling" crr="summarization." comments="Spelling / US style">summarisation.</edit>
    </introduction>   
</doc>