<?xml version="1.0" encoding="UTF-8"?>
<doc id="W18-1705" editor="A" format="WS" position="S" region="N">
    <title>
        <text>Large-scale spectral clustering using diffusion coordinates on landmark-based bipartite graphs</text>
    </title>
    <abstract>
        <text>Spectral clustering has received a lot of attention due to its ability to separate nonconvex,</text>
        <edit type="hyphenation" crr="non intersecting" comments="hyphenation - non- usually doesn't take a hyphen unless result is confusing">non-intersecting</edit>
        <text>manifolds, but its high computational complexity has significantly limited its applicability. Motivated by the document-term co-clustering framework by Dhillon (2001), we propose a landmark-based scalable spectral clustering approach in which we first use the selected landmark set and the given data to form a bipartite graph and then run a diffusion process on it to obtain a family of diffusion coordinates for clustering. We show that our proposed algorithm can be implemented based on very efficient operations on the affinity matrix between the given data and selected landmarks</text>
        <edit type="punctuation" crr="punctuation" comments="punctuation - no comma before dependent clause">,</edit>
        <edit type="grammar" crr="and thus is capable of" comments="grammar - missing conjunction - missing verb">thus capable of</edit>
        <text>handling large data. Finally, we demonstrate the excellent performance of our method by comparing</text>
        <edit type="grammar" crr="it" comments="grammar - missing pronoun"></edit>
        <text>with the state-of-the-art scalable algorithms on several benchmark data sets.</text>
    </abstract>   
    <introduction>
        <text>Given a data set [MATH] and a similarity function [MATH] such as the Gaussian radial basis function (RBF), spectral clustering (von Luxburg, 2007) first constructs a pairwise similarity matrix [MATH] and then uses the top eigenvectors of W (after a certain kind of normalization) to embed X into a low-dimensional space where k-means is employed to group the data into k clusters. Though mathematically quite simple, spectral clustering can easily adapt to nonconvex geometries and accurately separate various non-intersecting shapes. As a result, it has been successfully applied to many practical tasks, e.g., image segmentation (Shi and Malik, 2000) and document clustering (Dhillon, 2001), often significantly outperforming traditional methods (such as k-means). Furthermore, spectral clustering has a very rich theory (von Luxburg, 2007), with interesting connections to kernel k-means (Dhillon et al., 2004), random walk (Meila and Shi, 2001), graph cut (Shi and Malik, 2000) (and the underlying spectral graph theory (Chung, 1996)), and matrix perturbation analysis (Ng et al., 2001).</text>
        <text>\\ However, spectral clustering is known to suffer from a high computational cost associated with the n x n matrix W, especially when n is large. Consequently, there has been considerable effort to develop fast, approximate algorithms that can handle large data sets (Fowlkes et al., 2004; Yan et al., 2009; Sakai and Imiya, 2009; Wang et al., 2009; Chen and Cai, 2011; Wang et al., 2011; Tasdemir, 2012; Choromanska et al., 2013; Cai and Chen, 2015; Moazzen and Tasdemir, 2016; Chen, 2018). Interestingly, a considerable fraction of them use a landmark set to help reduce the computational complexity</text>
        <edit type="conciseness" crr="" comments="conciseness - this is redundant here, it's clear from context and the para intro">of spectral clustering</edit>
        <text> Specifically, they first find a small set of data representatives (called landmarks), [MATH] (with [MATH]), from the given data in X and then form an affinity matrix between X and Y (see Fig. 1):  [MATH]</text>
        <text>\\ Afterwards, different scalable methods use the matrix A in different ways to cluster the given data. For example, the column-sampling spectral clustering (cSPEC) algorithm (Wang et al., 2009) regards A as a column-reduced version of W and correspondingly</text>
        <edit type="grammar" crr="uses" comments="grammar - needs this form to agree with 'algorithm'">use</edit>
        <text>the left singular vectors of A to approximate the eigenvectors of W. However, they seem to consider only unnormalized spectral clustering, and it is unclear how they extend their technique to normalized spectral clustering (Shi and Malik, 2000; Ng et al., 2001). Another example is the landmark-based spectral clustering (LSC) algorithm (Cai and Chen, 2015) which uses a row-sparsified version of the matrix A as approximate sparse representations of the input data while bypassing the expensive dictionary learning and</text>
        <edit type="hyphenation" crr="sparse-coding" comments="hyphenation - compound adjective">sparse coding</edit>
        <text>tasks. It then applies the L1 normalization to each row of A, followed by a square-root L1 column normalization. This method empirically works quite well</text>
        <edit type="punctuation" crr=", but clearly," comments="punctuation - comma before independent clause;punctuation - comma after intro word">but clearly</edit>
        <text>there is a gap between its sparse- coding motivation and the actual implementation. A third example is the k-means-based approximate spectral clustering (KASP) algorithm (Yan et al., 2009) which first applies the k-means algorithm to partition the given data into m small clusters and then performs spectral clustering to divide their centroids (which are the landmark points) into k groups. Next, they extend the clustering of the landmarks to the original data by performing 1 nearest neighbor (1NN) classification. This algorithm runs very fast, but is sensitive to the k-means clusters as it aggressively reduces the given data to a small set of centroids</text>
        <text>\\ In this work, we propose a novel landmark-based scalable spectral clustering approach by adapting the co-clustering framework by Dhillon (Dhillon, 2001) for landmark-based clustering and combining it with diffusion maps (Coifman and Lafon, 2006). Specifically, with the given data [MATH] and a selected landmark set [MATH], we first construct a bipartite graph G2 with X and Y being the two parts, and form edges between each xi and its s nearest neighbors yj in the landmark set with weights [MATH]. We then compute the transition probabilities for all the vertices of G2 and use them to define a random walk on the bipartite graph, which (when being iterated forward) further generates a diffusion process on G2. We expect the resulting diffusion coordinates to be able to capture the global geometry of the clusters at different scales and, as a result, the connectivity of each cluster will be significantly strengthened (see Fig. 2). We will show that the diffusion coordinates may be computed directly from the n Ã— m matrix A = (aij). Lastly, we propose three different ways to use the diffusion coordinates for clustering the data in X (depending on the length of the random walk).</text>
        <text>\\ The rest of the paper is organized as follows. First, in Section 2, we review some necessary background. We then present our methodology in Section 3. Experiments are conducted in Section 4 to test our proposed algorithms. Finally, we conclude the paper in Section 5.</text>
    </introduction>   
</doc>