<?xml version="1.0" encoding="UTF-8"?>
<doc id="P16-1061" editor="B" format="Conf" position="S" region="N">
    <title>
        <text>Improving Coreference Resolution by Learning Entity-Level Distributed Representations</text>
    </title>
    <abstract>
        <edit type="word order" crr="Coreference resolution is a long-standing challenge in" comments="Word order; this is somewhat discretionary. It's better to lead with the subject of the paper. Also, it's better to use active tense.">A long-standing challenge in coreference resolution has been</edit>
        <text>the incorporation of entity-level information</text>
        <edit type="clarity" crr=", which consists of" comments="added for clarity">-</edit>
        <text>features defined over clusters of mentions instead of mention pairs. We present a</text>
        <edit type="punctuation" crr="neural-network-based" comments="punctuation; hyphens for compound adjective ending in 'based';added to create a compound adjective">neural network based</edit>
        <text>coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns when combining clusters is desirable. We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset, despite using few hand-engineered features.</text>
    </abstract>   
    <introduction>
        <text>\n\n Coreference resolution, the task of identifying which mentions in a text refer to the same real-world entity, is fundamentally a clustering problem. However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together (Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015).</text>
        <text>An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity. Such systems can take advantage of entity-level information, i.e., features</text>
        <edit type="clarity" crr="that occur" comments="added for clarity"></edit>
        <text>between clusters of mentions instead of between just two mentions. As an example</text>
        <edit type="grammar" crr="of" comments="">for</edit>
        <text>why this is useful, it is clear that the clusters {Bill Clinton} and {Clinton, she} are not referring to the same entity, but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent.</text>
        <text>\n\n Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters (Raghunathan et al., 2010; Durrett et al., 2013). In this work, we instead train a deep neural network to build distributed representations of pairs of coreference clusters. This captures entity-level information with a large number of learned, continuous features instead of a small number of hand-crafted categorical ones.</text>
        <text>Using the cluster-pair representations, our network learns when combining two coreference clusters is desirable. At test time, it builds up coreference clusters incrementally, starting with each mention in its own cluster and then merging a pair of clusters</text>
        <edit type="clarity" crr="at" comments="added for clarity"></edit>
        <text>each step. It makes these decisions with a novel easy-first cluster-ranking procedure that combines the strengths of cluster-ranking (Rahman and Ng, 2011) and easy-first (Stoyanov and Eisner, 2012) coreference algorithms.</text>
        <text>\n\n Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made. We address this by using a learning-to-search algorithm inspired by SEARN (Daum√© III et al., 2009) to train our neural network. This approach allows the model to learn which action (a cluster merge) available from the current state (a partially completed coreference clustering) will eventually lead to a high-scoring coreference partition.</text>
        <text>\n\n Our system uses little manual feature engineering, which means it is easily extended to multiple languages. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset. The cluster-ranking model significantly outperforms a mention-ranking model that does not use entity-level information. We also show that using an easy-first strategy improves the performance of the cluster-ranking model. Our final system achieves CoNLL F1 scores of 65.29 for English and 63.66 for Chinese, substantially outperforming other state-of-the-art systems.</text>
    </introduction>   
</doc>