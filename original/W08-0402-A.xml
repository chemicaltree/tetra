<?xml version="1.0" encoding="UTF-8"?>
<doc id="W08-0402" editor="A" format="WS" position="S" region="N">
    <title>
        <text>A Scalable Decoder for Parsing-based Machine Translation with Equivalent Language Model State Maintenance</text>
    </title>
    <abstract>
        <text>We describe a scalable decoder for parsing-based machine translation. The decoder is written in</text>
        <edit type="spelling" crr="Java" comments="spelling - Java is not all capped">JAVA</edit>
        <text>and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language- model integration, beam- and cube-pruning, and unique k-best extraction. Additionally,</text>
        <edit type="style" crr="we exploit parallel and distributed computing techniques" comments="style - removing unneeded passive voice">parallel and distributed computing techniques are exploited</edit>
        <text>to make</text>
        <edit type="clarity" crr="the decoder" comments="clarity - the referent for 'it' is too far back to be clear">it</edit>
        <text>scalable. We also propose an algorithm to maintain equivalent language model states that exploits the back-off property of m-gram language models: instead of maintaining a separate state for each distinguished sequence of “state” words, we merge multiple states that can be made equivalent for language model probability calculations due to back-off. We demonstrate experimentally that our decoder is more than 30 times faster than a baseline decoder written in</text>
        <edit type="spelling" crr="Python" comments="spelling - Python is not all capped">PYTHON</edit>
        <text>. We propose to release our decoder as an open-source toolkit.</text>
    </abstract>   
    <introduction>
        <edit type="clarity" crr="Work on large-scale " comments="clarity - it's not the translation that has made progress, it's the work or the field">Large-scale</edit>
        <text>parsing-based statistical machine translation (MT) has made remarkable progress in the last few years. The systems being developed differ in whether they use source- or target-language syntax. For instance, the hierarchical translation system of Chiang (2007) extracts a synchronous grammar from pairs of strings, Quirk et al. (2005), Liu et al. (2006) and Huang et al. (2006) perform syntactic analyses in the source -language, and Galley et al. (2006) use target-language syntax.</text>
        <text>A critical component in parsing-based MT systems is the decoder, which is complex to implement and scale up. Most of the systems described above employ tailor-made, dedicated decoders that are not open-source , which results in a high barrier to entry for other researchers in the field. However, with the algorithms proposed in (Huang and Chiang, 2005; Chiang, 2007; Huang and Chiang, 2007), it is possible to develop a general-purpose decoder that can be used by all the parsing-based systems. In this paper, we describe an important first-step towards an extensible, general-purpose, scalable, and open-source parsing-based MT decoder. Our decoder is written in</text>
        <edit type="spelling" crr="Java" comments="spelling - Java is not all capped">JAVA</edit>
        <text>and implements all the essential algorithms described in Chiang (2007): chart-parsing, m-gram language model integration, beam- and cube-pruning, and unique k-best extraction. Additionally,</text>
        <edit type="style" crr="we exploit parallel and distributed computing techniques" comments="style - removing unneeded passive voice">parallel and distributed computing techniques are exploited</edit>
       <text>to make</text>
        <edit type="clarity" crr="the decoder" comments="clarity - the referent for 'it' is too far back to be clear">it</edit>
        <text>scalable.</text>
        <text>\\ Straightforward integration of an m-gram language model (LM) into a parsing-based decoder substantially increases its computational complexity. Therefore, it is important to develop efficient methods for LM integration. We propose an algorithm to maintain equivalent LM states by exploiting the back-off property of m-gram LMs. Specifically, instead of maintaining a separate state for each distinguished sequence of “state” words, we merge multiple states that can be made equivalent for LM calculations by anticipating such back-off.</text>
        <text>\\ We demonstrate experimentally that our decoder is 38 times faster than a previous decoder written in</text>
        <edit type="spelling" crr="Python" comments="spelling - Python is not all capped">PYTHON</edit>
        <text>. Furthermore, the distributed computing permits improving translation quality via large-scale LMs. We have successfully</text>
        <edit type="spelling" crr="used" comments="spelling - need the past tense here to agree with 'have'">use</edit>
        <text>our decoder to translate about a million sentences in a parallel corpus for large-scale discriminative training experiments.</text>
    </introduction>   
</doc>