<?xml version="1.0" encoding="UTF-8"?>
<doc id="W09-2101" editor="B" format="WS" position="NS" region="N">
    <title>
        <text>Automatic Assessment of Spoken Modern Standard Arabic</text>
    </title>
    <abstract>
        <text>Proficiency testing is an important ingredient in successful language</text>
        <edit type="word choice" crr="instruction" comments="">teaching</edit>
        <text>. However, repeated testing for course placement,</text>
        <edit type="word choice" crr="during" comments="">over</edit>
        <text>the course of instruction or for certification</text>
        <edit type="punctuation" crr="," comments=""></edit>
        <text>can be time-consuming and costly. We present the design and validation of the Versant Arabic Test, a fully automated test of spoken Modern Standard Arabic</text>
        <edit type="punctuation" crr="" comments="">,</edit>
        <text>hat evaluates test-takers' facility in listening and speaking. Experimental data shows the test to be highly reliable (test-retest r=0.97) and</text>
        <edit type="word choice;clarity" crr="strongly predictive of" comments="word choice; words added for clarity">to strongly predict</edit>
        <text>performance on the ILR OPI (r=0.87), a standard interview test that assesses oral proficiency.</text>
    </abstract>   
    <introduction>
        <text>Traditional high-stakes testing of spoken proficiency often evaluates the test-taker's ability to accomplish communicative tasks in a conversational setting. For example, learners may introduce themselves, respond to requests for information, or accomplish daily tasks in a role-play.</text>
        <text>\\ Testing oral proficiency in this way can be time-consuming and costly, since at least one trained interviewer is needed</text>
        <edit type="word choice;clarity" crr="to test each test taker." comments="word choice; it seems like this type of testing could be used outside a school setting.">for each student.</edit>
        <text>For example, the standard oral proficiency test used by</text>
        <edit type="grammar" crr="" comments="">the</edit>
        <text>United States government agencies (the Interagency Language Roundtable Oral Proficiency Interview or ILR OPI) is usually administered by two certified interviewers</text>
        <edit type="clarity" crr="and requires" comments="">for</edit>
        <text>approximately 30-45 minutes per candidate.</text>
        <text>\\ The</text>
        <edit type="word choice;clarity" crr="significant" comments="">great</edit>
        <text>effort involved in oral proficiency interview (OPI) testing makes automated testing an attractive alternative.</text>
        <edit type="readability" crr="Research into fully automated scoring of speaking ability has been reported" comments="">Work has been reported on fully automated scoring of speaking ability</edit>
        <text>(e.g., Bernstein and Barbier, 2001; Zechner et al., 2007, for English; Balogh and Bernstein, 2007, for English and Spanish). Automated testing systems</text>
        <edit type="word choice" crr="are not aimed at simulating" comments="">do not aim to simulate</edit>
        <edit type="grammar" crr="conversations" comments="">a conversation</edit>
        <text>with</text>
        <edit type="grammar" crr="test-takers" comments="">the test-taker</edit>
        <text>and therefore do not directly observe interactive human communication. Bernstein and Barbier (2001) describe a system that</text>
        <edit type="word choice" crr="could" comments="">might</edit>
        <text>be used</text>
        <edit type="word choice" crr="to qualify" comments="">in qualifying</edit>
        <text>simultaneous interpreters; Zechner et al. (2007) describe an automated scoring system that assesses performance according to the TOEFL iBT speaking rubrics. Balogh and Bernstein (2007) focus on evaluating facility in a spoken language, a separate test construct that relates to oral proficiency.</text>
        <text>\\ “Facility in a spoken language” is defined as “the ability to understand a spoken language on everyday topics and to respond appropriately and intelligibly at a native-like conversational pace” (Balogh and Bernstein, 2007, p. 272). This ability is assumed to underlie high performance in communicative settings</text>
        <edit type="punctuation" crr="" comments="">,</edit>
        <text>since learners have to understand their interlocutors correctly and</text>
        <edit type="readability" crr="be able to efficiently respond to them in real time." comments="">efficiently in real time to be able to respond.</edit>
        <edit type="clarity;word choice" crr="Equally important in demonstrating their communication abilities, test takers must" comments="">Equally learners have to</edit>
        <text>be able to formulate and articulate</text>
        <edit type="grammar" crr="comprehensible answers" comments="">a comprehensible answer</edit>
        <text>without undue delay. Testing for oral proficiency, on the other hand, conventionally includes additional aspects such as correct interpretation of the pragmatics of the conversation, socially and culturally appropriate wording and content, and knowledge of the subject matter under discussion.</text>
        <text>\\ In this paper, we describe the design and validation of the Versant Arabic Test (VAT), a fully automated test of</text>
        <edit type="word choice" crr="spoken facility in" comments="">facility with spoken</edit>
        <text>Modern Standard Arabic (MSA). Focusing on facility rather than communication-based oral proficiency enables the creation of an efficient yet informative automated test of listening and speaking ability. The automated test can be administered over the telephone or on a computer in approximately 17 minutes. Despite its much shorter format and constrained tasks, test-taker scores on the VAT strongly correspond to their scores</text>
        <edit type="grammar" crr="on the" comments="">from an</edit>
        <text>ILR Oral Proficiency Interview.</text>
        <text>\\ The paper is structured as follows</text>
        <edit type="punctuation" crr="." comments="">:</edit>
        <text>After reviewing related work</text>
        <edit type="readability;consistency" crr="(Section 2)" comments="I'm assuming this is where they describe it."></edit>
        <text>, we describe Modern Standard Arabic and introduce the test construct (i.e., what the test is intended to measure) in detail (Section 3). We then describe the structure and development of the VAT</text>
        <edit type="readability;consistency" crr="(Section 4) and present evidence for its reliability and validity (Section 5)." comments="">in Section 4 and present evidence for its reliability and validity in Section 5.</edit>
    </introduction>   
</doc>