<?xml version="1.0" encoding="UTF-8"?>
<doc id="W17-2620" editor="A" format="WS" position="S" region="NN">
    <title>
        <text>Transfer Learning for Speech Recognition on a Budget</text>
    </title>
    <abstract>
        <text>End-to-end training of automated speech recognition (ASR) systems requires massive data and compute resources. We explore transfer learning based on model adaptation as an approach for training ASR models under constrained GPU memory, throughput</text>
        <edit type="punctuation" crr="," comments="punctuation - adding serial comma - can be style specific but in this case I think it's needed for disambiguation"></edit>
        <text>and training data. We conduct several systematic experiments adapting a Wav2Letter convolutional neural network, originally trained for English ASR, to the German language. We show that this technique allows faster training on consumer-grade resources while requiring less training data in order to achieve the same accuracy, thereby lowering the cost of training ASR models in other languages. Model introspection revealed that small adaptations to the network's weights were sufficient for good performance, especially for inner layers.</text>
    </abstract>   
    <introduction>
        <text>Automated speech recognition (ASR) is the task of translating spoken language to text in real-time. Recently, end-to-end deep learning approaches have surpassed previously predominant solutions based on Hidden Markov Models. In an influential paper, Amodei et al. (2015) used convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to redefine the state of the art. However, Amodei et al. (2015) also highlighted the shortcomings of the deep learning approach. Performing forward and backward propagation on complex deep networks in a reasonable amount of time requires expensive specialized hardware. Additionally, in order to set the large number of parameters of a deep network properly, one needs to train on large amounts of audio recordings</text>
        <edit type="flow" crr=", which, most" comments="flow - joining these sentences for better flow. It's also slightly shorter">. Most</edit>
        <text>of the time,</text>
        <edit type="redundancy" crr="" comments="">the recordings</edit>
        <text>need to be transcribed by hand. Such data in adequate quantities is currently available for few languages other than English.</text>
        <text>\\ We propose an approach combining two methodologies to address these shortcomings. Firstly, we use a simpler model with a lower resource footprint. Secondly, we apply a technique called transfer learning to significantly reduce the amount of non-English training data needed to achieve competitive accuracy in an ASR task. We investigate the efficacy of this approach on the specific example of adapting a CNN-based end-to-end model, originally trained on English, to recognize German speech. In particular, we freeze the parameters of its lower layers while retraining the upper layers on a German corpus which is smaller than its English counterpart. We expect this approach to yield the following three improvements. Taking advantage of the representation learned by the English model will lead to shorter training times compared to training from scratch. Relatedly, the model trained using transfer learning</text>
        <edit type="parallelism;word choice" crr="will require" comments="parallelism / word choice - this is a list of improvements the author expects to see, so we want the future sense of 'will require' here. That also aligns with the form of the prior sentence">require</edit>
        <text>less data for an equivalent score than a German-only model. Finally, the more layers we freeze the fewer layers we</text>
        <edit type="parallelism" crr="will" comments="parallelism - as above"></edit>
        <text>need to back-propagate through during training</text>
        <edit type="clarity" crr="; thus," comments="clarity - joining these two sentences to make it clear the both form the third improvement, rather than there being four.">. Thus</edit>
        <text>we expect to see a decrease in GPU memory usage since we do not have to maintain gradients for all layers.</text>
        <text>\\ This paper is structured as follows. Section 2 gives an overview of other transfer learning approaches to ASR tasks. Details about our implementation of the Wav2Letter model and how we trained it can be found in Section 3. The data we used and how we preprocessed it is described in Section 4. After a short introduction of the performed experiments in Section 5, we present and discuss the results in Section 6, followed by a conclusion in Section 7.</text>
    </introduction>   
</doc>