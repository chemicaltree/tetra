<?xml version="1.0" encoding="UTF-8"?>
<doc id="P11-1148" editor="C" format="Conf" position="NS" region="N">
    <title>
        <text>Latent Semantic Word Sense Induction and Disambiguation</text>
    </title>
    <abstract>
        <text>In this paper, we present a unified model for the automatic induction of word senses from text</text>
        <edit type="punctuation" crr="" comments="Punctuation / comma not required in a list of two clauses">,</edit>
        <text>the subsequent disambiguation of particular word instances using the automatically- extracted sense inventory.</text>
        <edit type="word choice;clarity" crr="Both the induction" comments="">The induction</edit>
        <text>step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The intuition is that a particular sense is associated with a particular topic, so that different senses can be discriminated through their association with particular topical dimensions</text>
        <edit type="punctuation" crr=". In" comments="Punctuation / end sentence">; in</edit>
        <text>a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions.</text>
        <edit type="repetitiveness" crr="Our" comments="Word usage / repetition of 'the'">The</edit>
        <text>model is evaluated on the SEMEVAL-2010 word sense induction and disambiguation task, on which it reaches state-of-the-art results.</text>
    </abstract>   
    <introduction>
        <text>Word sense induction (WSI) is the task of automatically identifying the senses of words in texts, without the need for handcrafted resources or manually- annotated data. The manual construction of a sense inventory is a tedious and time-consuming job, and the result is highly dependent on the annotators and the domain at hand. By applying an automatic procedure, we are able to only extract the senses that are objectively present in a particular corpus, and it allows for the sense inventory to be</text>
        <edit type="flow" crr="adapted to a new domain in a straightforward manner." comments="Paraphrase / restructure sentence">straightforwardly adapted to a new domain。</edit>
        <text>\n\n Word sense disambiguation (WSD), on the other hand, is the</text>
        <edit type="hyphenation" crr="closely-related" comments="">closely related</edit>
        <text>task of assigning a sense label to a particular instance of a word in context, using an existing sense inventory. The bulk of WSD algorithms up</text>
        <edit type="spelling" crr="untill" comments="">till</edit>
        <text>now use pre-defined sense inventories (such as WordNet) that often contain fine-grained sense distinctions, which poses serious problems for computational semantic processing (Ide and Wilks, 2007). Moreover, most WSD algorithms take a supervised approach, which requires a significant amount of</text>
        <edit type="hyphenation" crr="manually-annotated" comments="">manually annotated</edit>
        <text>training data</text>
        <edit type="punctuation" crr="." comments=""></edit>
        <text>\n\n The model presented here induces the senses of words in a fully unsupervised way</text>
        <edit type="readability" crr=". Subsequently, it" comments="Readability / end long sentences wherever possible">, and subsequently</edit>
        <text>uses the induced sense inventory for the unsupervised disambiguation of particular occurrences of words. The induction step and the disambiguation step are based on the same principle: words and contexts are mapped to a limited number of topical dimensions in a latent semantic word space. The</text>
        <edit type="style" crr="core focus of exploration" comments="Register / 'key idea' is correct, but the alternative is better for academic style and more precise">key idea</edit>
        <text>is that the model combines tight,</text>
        <edit type="hyphenation" crr="synonym-like" comments="">synonym like</edit>
        <text>similarity (based on dependency relations) with broad, topical similarity (based on a large ‘bag of words’ context window). The intuition in this is that the dependency features can be disambiguated by the topical dimensions identified by the broad contextual features</text>
        <edit type="punctuation" crr=". In" comments="Punctuation / end sentence">; in</edit>
        <text>a similar vein, a particular instance of a word can be disambiguated by determining its most important topical dimensions (based on the instance’s context words).</text>
        <edit type="repetitiveness" crr="\n\n This" comments="Word usage / frequent repetition of 'the'">\n\n The</edit>
        <text>paper is organized as follows. Section 2 presents some previous research on distributional similarity and word sense induction. Section 3 gives an overview of our method for word sense induction and disambiguation.</text>
        <edit type="repetitiveness" crr="In Section 4, we provide" comments="Paraphrase / frequent repetition of 'Section'">Section 4 provides</edit>
        <text>a quantitative evaluation and comparison to other algorithms in the framework of the SEMEVAL-2010 word sense induction and disambiguation (WSI/WSD) task. The</text>
        <edit type="style" crr="final" comments="">last</edit>
        <text>ection draws conclusions, and lays out a number of future research directions.</text>
    </introduction>   
</doc>