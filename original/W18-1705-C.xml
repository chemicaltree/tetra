<?xml version="1.0" encoding="UTF-8"?>
<doc id="W18-1705" editor="C" format="WS" position="S" region="N">
    <title>
        <edit type="capitalization" crr="Large-Scale Spectral Clustering Using Diffusion Coordinates on Landmark-Based Bipartite Graphs" comments="">Large-scale spectral clustering using diffusion coordinates on landmark-based bipartite graphs</edit>
    </title>
    <abstract>
        <text>Spectral clustering has received</text>
        <edit type="style" crr="much" comments="Style / consider replacing in order to shorten a long sentence.">a lot of</edit>
        <text>attention due to its ability to separate nonconvex, non-intersecting manifolds</text>
        <edit type="readability" crr=". However," comments="Readability/start new sentence">, but</edit>
        <text>its high computational complexity has significantly limited its applicability.</text>
        <edit type="word choice" crr="Inspired" comments="Meaning / 'inspired' is a more precise term in this context">Motivated</edit>
        <text>by the document-term co-clustering framework by Dhillon (2001), we propose a landmark-based scalable spectral clustering approach</text>
        <edit type="readability" crr=". In this approach," comments="Readability/end long sentence.;restructure to follow on from previous sentence">in which</edit>
        <text>we first use the selected landmark set and the given data to form a bipartite graph, and then run a diffusion process on it to obtain a family of diffusion coordinates for clustering. We show that our proposed algorithm can be implemented based on very efficient operations on the affinity matrix between the given data and selected landmarks,</text>
        <edit type="readability" crr="and is therefore" comments="Readability / replace to improve meaning">thus</edit>
        <text>capable of handling large</text>
        <edit type="clarity;readability" crr="amounts of" comments="Readability / insert to improve meaning"></edit>
        <text>data. Finally, we demonstrate the excellent performance of our method by comparing</text>
        <edit type="clarity" crr="it" comments="Meaning / insert to clarify"></edit>
        <text>with state-of-the-art scalable algorithms on several benchmark data sets.</text>
    </abstract>   
    <introduction>
        <text>Given a data set [MATH] and a similarity function [MATH] such as the Gaussian radial basis function (RBF), spectral clustering (von Luxburg, 2007) first constructs a pairwise similarity matrix [MATH] and then uses the top eigenvectors of W (after certain kind of normalization) to embed X into a low-dimensional space where k-means is employed to group the data into k clusters.</text>
        <edit type="word choice" crr="Althoug" comments="Meaning / 'although' is more accurate in this contex">Though</edit>
        <text>mathematically quite simple, spectral clustering can easily adapt to nonconvex geometries and accurately separate various non-intersecting shapes. As a result, it has been successfully applied to</text>
        <edit type="style" crr="numerous" comments="Style / 'many' is correct but the alternative is better in academic writing">many</edit>
        <text>practical tasks, e.g., image segmentation (Shi and Malik, 2000) and document clustering (Dhillon, 2001), often significantly outperforming traditional methods (such as k-means). Furthermore, spectral clustering has a very rich theory (von Luxburg, 2007), with interesting connections to kernel k-means (Dhillon et al., 2004), random walk (Meila and Shi, 2001), graph cut (Shi and Malik, 2000) (and the underlying spectral graph theory (Chung, 1996)), and matrix perturbation analysis (Ng et al., 2001).</text>
        <text>\\ However, spectral clustering is known to suffer from a high computational cost associated with the n x n matrix W, especially when n is large. Consequently, there has been considerable effort to develop fast, approximate algorithms</text>
        <edit type="word choice;clarity" crr="capable of handling" comments="Meaning / what you wrote is correct, but 'capable of...' infers more meaning">that can handle</edit>
        <text>large data sets (Fowlkes et al., 2004; Yan et al., 2009; Sakai and Imiya, 2009; Wang et al., 2009; Chen and Cai, 2011; Wang et al., 2011; Tasdemir, 2012; Choromanska et al., 2013; Cai and Chen, 2015; Moazzen and Tasdemir, 2016; Chen, 2018).</text>
        <edit type="word choice" crr="Surprisingly" comments="Meaning/the proposed alternative is probably more accurate in this context">Interestingly</edit>
        <text>, a considerable fraction of them use a landmark set to help reduce the computational complexity of spectral clustering. Specifically, they first find a small set of data representatives (called landmarks), [MATH] (with [MATH]), from the given data in X, and then form an affinity matrix between X and Y (see Fig. 1): [MATH]</text>
        <text>\\ Afterwards, different scalable methods use the matrix A in different ways to cluster the given data. For example, the column-sampling spectral clustering (cSPEC) algorithm (Wang et al., 2009) regards A as a column-reduced version of W and correspondingly use the left singular vectors of A to approximate the eigenvectors of W. However, they seem to consider only unnormalized spectral clustering, and it is unclear how they extend their technique to normalized spectral clustering (Shi and Malik, 2000; Ng et al., 2001). Another example is the landmark-based spectral clustering (LSC) algorithm (Cai and Chen, 2015) which uses a row-sparsified version of the matrix A as approximate sparse representations of the input data while bypassing the expensive dictionary learning and sparse coding tasks. It then applies the L1 normalization to each row of A, followed by a square-root L1 column normalization.</text>
        <edit type="readability" crr="Although this method empirically works quite well, there is clearly a gap" comments="Punctuation/insert comma to break up long sentence/sequence; Readability/your construction is correct, however, introduce the 'but' upfront in the sentence to set the reader up for the expectation">This method empirically works quite well but clearly there is a gap</edit>
        <text>between its sparse coding motivation and the actual implementation. A third example is the k-means-based approximate spectral clustering (KASP) algorithm (Yan et al., 2009) which first applies the k-means algorithm to partition the given data into m small clusters and then performs spectral clustering to divide their centroids (which are the landmark points) into k groups. Next, they extend the clustering of the landmarks to the original data by performing 1 nearest neighbor (1NN) classification. Although this algorithm runs very fast, it is sensitive to the k-means clusters as it aggressively reduces the given data to a small set of centroids.</text>
        <text>\\ In this work, we propose a novel landmark-based scalable spectral clustering approach by adapting the</text>
        <edit type="readability;repetitiveness" crr="Dhillon's co-clustering framework" comments="Readability/repetition of 'by'/delete">co-clustering framework by Dhillon</edit>
        <text>(Dhillon, 2001) for landmark-based clustering and combining it with diffusion maps (Coifman and Lafon, 2006). Specifically, with the given data [MATH] and a selected landmark set [MATH], we first construct a bipartite graph G2 with X and Y being the two parts, and then form edges between each xi and its s nearest neighbors yj in the landmark set with weights [MATH]. We then compute the transition probabilities for all the vertices of G2 and use them to define a random walk on the bipartite graph, which (when being iterated forward) further generates a diffusion process on G2. We expect the resulting diffusion coordinates to be able to capture the global geometry of the clusters at different scales and, as a result, the connectivity of each cluster will be significantly strengthened (see Fig. 2). We will show that the diffusion coordinates may be computed directly from the n Ã— m matrix A = (aij). Lastly, we propose three different ways to use the diffusion coordinates for clustering the data in X (depending on the length of the random walk).</text>
        <text>\\ The</text>
        <edit type="style" crr="remaining" comments="Register/'rest' is correct, but alternative better for academic text">rest</edit>
        <text>of the paper is organized as follows. First, in Section 2, we review some necessary background. We then present our methodology in Section 3. Experiments are conducted in Section 4 to test our proposed algorithms. Finally, we conclude the paper in Section 5.</text>
    </introduction>   
</doc>