<?xml version="1.0" encoding="UTF-8"?>
<doc id="P03-1006" editor="A" format="Conf" position="NS" region="N">
    <title>
        <text>Generalized Algorithms for Constructing Statistical Language Models</text>
    </title>
    <abstract>
        <text>Recent text and speech processing applications such as speech mining raise new and more general problems related to the construction of language models. We present</text>
        <edit type="style;readability" crr="several new and efficient algorithms to address these more general problems, describe the algorithms in detail," comments="style /readability - by pulling the object of all of these phrases up closer to the first verb, we can make this sentence flow a lot better.">and describe in detail, several new and efficient algorithms to address these more general problems</edit>
        <text>and report experimental results demonstrating their usefulness. We give an algorithm for</text>
        <edit type="style;word order" crr="efficiently computing" comments="style / readability - it's more natural for the adverb to come first here">computing efficiently</edit>
        <text>the expected counts of any sequence in a word lattice</text>
        <edit type="readability" crr="which has been" comments="readability - grammatically, the original is ok, but it causes stumbles over whether 'output' is a verb or an object modified by 'word lattice'"></edit>
        <text>output by a speech recognizer or any arbitrary weighted automaton; describe a new technique for creating exact representations of n-gram language models by weighted automata whose size is practical for offline use even for a vocabulary size of about 500,000 words and an n-gram order n = 6; and present a simple and more general technique for constructing class-based language models that allows each class to represent an arbitrary weighted automaton. An efficient implementation of our algorithms and techniques has been incorporated in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities.</text>
    </abstract>   
    <introduction>
        <text>Statistical language models are crucial components of many modern natural language processing systems such as speech recognition, information extraction, machine translation,</text>
        <edit type="word choice" crr="and" comments="word choice - this is an inclusive list, so 'and' is more appropriate than 'or'">or</edit>
        <text>document classification. In</text>
        <edit type="word choice;clarity" crr="these" comments="word choice / clarity - I think it's not accurate to say 'all' cases as this would include non-statistical model">all</edit>
        <text>cases, a language model is used in combination with other information sources to rank alternative hypotheses by assigning them some probabilities. There are classical techniques for constructing language models such as n-gram models with various smoothing techniques (see Chen and Goodman (1998) and the references therein for a survey and comparison of these techniques).</text>
        <text>\\ In some recent text and speech processing applications, several new and more general problems arise that are related to the construction of language models. We present new and efficient algorithms to address these more general problems.</text>
        <text>\\ Counting. Classical language models are constructed by deriving statistics from large input texts. In speech mining applications or for adaptation purposes, one often needs to construct a language model based on the output of a speech recognition system. But, the output of a recognition system is not just text. Indeed, the word error rate of conversational speech recognition systems is still too high in many tasks to rely only on the one-best output of the recognizer. Thus, the word lattice output by speech recognition systems is used instead because it contains the correct transcription in most cases.</text>
        <text>\\ A word lattice is a weighted finite automaton (WFA)</text>
        <edit type="readability;clarity" crr="that is" comments="readability / clarity - as above, this helps disambiguate 'output' as verb vs noun"></edit>
        <text>output by the recognizer for a particular utterance. It</text>
        <edit type="style;word order" crr="typically contains" comments="style / readability - as above, it's smoother with the adverb first">contains typically</edit>
        <text>a very large set of alternative transcription sentences for that utterance with the corresponding weights or probabilities. A necessary step for constructing a language model based on a word lattice is to derive the statistics for any given sequence from the lattices, or WFAs, output by the recognizer. This cannot be done by simply enumerating each path of the lattice and counting the number of occurrences of the sequence considered in each path since the number of paths of even a small automaton may be more than four billion. We present a simple and efficient algorithm for computing the expected count of any given sequence in a WFA and report experimental results demonstrating its efficiency.</text>
        <text>\\ Representation of language models by WFAs. Classical n-gram language models admit a natural representation by WFAs in which each state encodes a left context of width less than n. However, the size of that representation makes it impractical for offline optimizations such as those used in large-vocabulary speech recognition or general information extraction systems. Most offline representations of these models are based instead on an approximation to limit their size. We describe a new technique for creating an exact representation of n-gram language models by WFAs whose size is practical for offline use even in tasks with a vocabulary size of about 500,000 words and for n = 6.</text>
        <text>\\ Class-based models. In many applications, it is natural and convenient to construct class-based language models, that is, models based on classes of words (Brown et al., 1992). Such models are also often more robust since they may include words that belong to a class but that were not found in the corpus. Classical class-based models are based on simple classes such as a list of words. But new clustering algorithms allow one to create more general and more complex classes that may be regular languages. Very large and complex classes can also be defined using regular expressions. We present a simple and more general approach to class-based language models based on general weighted context-dependent rules (Kaplan and Kay, 1994; Mohri and Sproat, 1996). Our approach allows us to deal efficiently with more complex classes such as weighted regular languages.</text>
        <text>\\ We have fully implemented the algorithms just mentioned and incorporated them in a general software library for language modeling, the GRM Library, that includes many other text and grammar processing functionalities (Allauzen et al., 2003). In the following, we will present</text>
        <edit type="style;word order;clarity" crr="these algorithms in detail" comments="style / readability - bringing the object closer to the verb helps with clarity and flow">in detail these algorithms</edit>
        <text>and briefly describe the corresponding GRM utilities.</text>
    </introduction>   
</doc>