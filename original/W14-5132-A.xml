<?xml version="1.0" encoding="UTF-8"?>
<doc id="W14-5132" editor="A" format="WS" position="S" region="NN">
    <title>
        <text>Discriminating Neutral and Emotional Speech using Neural Networks</text>
    </title>
    <abstract>
        <text>In this paper, we address the issue of speaker-specific emotion detection (neutral vs emotional) from speech signals with models for neutral speech as reference. As emotional speech is produced by the human speech production mechanism, the emotion information is expected to lie in the features of both the excitation source and the vocal tract system. Linear Prediction</text>
        <edit type="capitalization" crr="Residual" comments="capitalization - I'm not convinced this term needs to be capitalized, but if it is, the whole thing should be.">residual</edit>
        <text>is used as the excitation source component and Linear Prediction Coefficients as the vocal tract system component. A pitch synchronous analysis is performed. Separate Autoassociative Neural Network</text>
        <edit type="style" crr="(AANN)" comments="style - adding acronym that is used later"></edit>
        <text>models are developed to capture the information specific to neutral speech</text>
        <edit type="punctuation" crr="" comments="punctuation - no reason for comma here before prepositional phrase">,</edit>
        <text>from the excitation and</text>
        <edit type="grammar" crr="" comments="grammar - don't need the second article since the 'system components' phrase is shared">the</edit>
        <text>vocal tract system components. Experimental results show that the excitation source carries more information than the vocal tract system. The accuracy of the neutral vs emotional classification using excitation source information is 91%, which is 8% higher than the accuracy obtained using vocal tract system information. The Berlin EMO-DB database is used in this study. It is observed that the proposed emotion detection system provides an improvement of approximately 10% using excitation source features and 3% using vocal tract system features over the recently proposed emotion detection</text>
        <edit type="grammar;readability" crr="technique" comments="grammar and readability - need a term here that emotion detection modifies. This is a good alternative to repeating 'system'"></edit>
        <text>which uses</text>
        <edit type="grammar" crr="" comments="grammar - article isn't needed here and doesn't read well">the</edit>
        <text>energy and pitch contour modeling with functional data analysis.</text>
    </abstract>   
    <introduction>
        <text>Speech is produced by the human speech production mechanism, and it carries the signature of the speaker</text>
        <edit type="clarity" crr=":" comments="clarity - i think this list following breaks down the signature, but i would query the author to be sure. if not, then the clause needs reworded.">,</edit>
        <text>message, language, dialect, age, gender, context, culture, and state of the speaker such as emotions or expressive states. Extraction of these elements of information from the speech signal depends on identification and extraction of relevant acoustic parameters. Information present in the speech signal, including</text>
        <edit type="grammar" crr="the" comments="grammar - missing article"></edit>
        <text>emotional state of a speaker, has</text>
        <edit type="grammar" crr="" comments="grammar - do not need pronoun here">its</edit>
        <text>impact on the performance of speech systems (Athanaselis et al., 2005).</text>
        <text>\\ In this study, emotion detection refers to, identification of whether the speech is neutral or emotional. Emotion recognition refers to determining the category of emotion, i.e.,</text>
        <edit type="style" crr="angry" comments="style - making form of words the same for parallelism">angrer</edit>
        <text>, happy, sad, etc. The focus in this study is on detection of the presence of emotional state of a speaker with the use of reference models for neutral speech. Motivated by a broad range of commercial applications, automatic emotion recognition from speech has gained increasing research attention over the past few years. Some of the applications for emotion recognition systems are in the fields of health care</text>
        <edit type="grammar" crr="and" comments="grammar - no comma in list of two items. The 'and also for...' doesn't fit as part of this list">,</edit>
        <text>call centre services and also for developing speech systems such as automatic speech recognizers (ASR) to improve the performance of dialogue systems (Athanaselis et al., 2005; Mehu and Scherer, 2012; Cowie et al., 2001; Morrison et al., 2007).</text>
        <text>\\ Extraction of features from speech signals that characterize the</text>
        <edit type="repetitiveness" crr="emotional content" comments="repetitiveness - don't need to repeat 'speech' here.Also changed emotion to adjective form for smoother reading and removed unnecessary comma">emotion content of speech,</edit>
        <text>and at the same time do not depend on the lexical content is an important issue in emotion recognition (Schuller et al., 2010; Luengo et al., 2010; Scherer, 2003; Williams and Stevens, 1972; Murray and Arnott, 1993; Lee and Narayanan, 2005). From (Schuller et al., 2010; Hassan and Damper, 2012; Schuller et al., 2013; Schuller et al., 2011), it is observed that there is no clear understanding on what type of features can be used for the emotion recognition task.</text>
        <edit type="grammar" crr="The brute" comments="grammar - sinning article">brute</edit>
        <text>force approach involves extracting as many features as possible</text>
        <edit type="punctuation" crr="" comments="punctuation - comma not needed before dependent clause">,</edit>
        <text>and using these in the experiments, sometimes using feature selection mechanisms to choose the appropriate subset of features (Schuller et al., 2013; Schuller et al., 2011; Schuller et al., 2009; Zeng et al., 2009). These features can be broadly classified as prosodic features (pitch, intensity, duration), voice quality features (jitter, shimmer, harmonic to noise ratio (HNR)), spectral features (Mel Frequency Cepstral Coefficients (MFCCs), Linear Prediction Cepstral Coefficients (LPCCs)), and their statistics such as mean, variance, minimum, maximum, range (Zeng et al., 2009; Schuller et al., 2011; Schuller et al., 2009; ?; Eyben et al., 2012). A limitation of this approach is the assumption that every segment in the utterance is equally important. Studies have shown that emotional information is not uniformly distributed in time (Jeon et al., 2011; Lee et al., 2011; Shami and Verhelst, 2007).</text>
        <text>\\ In (Busso et al., 2009; Bulut and Narayanan, 2008; Arias et al., 2014; Arias et al., 2013; Busso et al., 2007), authors observed that</text>
        <edit type="grammar" crr="" comments="grammar - no article since this is plural and a class rather than a specific item">a</edit>
        <text>robust neutral speech models can be useful in contrasting different emotions expressed in speech.</text>
        <edit type="grammar" crr="An emotion" comments="grammar - missing article">Emotion</edit>
        <text>detection study was made by creating acoustic spectral features of neutral speech with HMMs (Busso et al., 2007). In (Busso et al., 2009), authors used the pitch features of neutral speech to discriminate the emotions using the Kullback-Leibler distance. It was observed that gross pitch contour statistics such as mean, minimum, maximum and range are prominent than pitch shape. Recently, emotion detection is performed using functional data analysis (FDA) (Arias et al., 2014; Arias et al., 2013). In this approach, pitch and energy contours of neutral speech utterance are modeled using FDA. In testing, pitch and energy contours are projected onto the reference bases, and their projections are used to discriminate neutral and emotional speech. Similar studies were made to model the shape of the pitch contour of emotional speech by analyzing the rising and falling movements (Astrid and Sendlmeier, 2010). One limitation with the studies (Arias et al., 2014; Arias et al., 2013) is that, all the utterances should be temporally aligned with the Dynamic Time Warping</text>
        <edit type="style" crr=", which" comments="style - making a smoother join between the clauses.">and it</edit>
        <text>may not be realistic for most of the situations.</text>
        <text>\\ Here, we propose an approach based on AANNs (Yegnanarayana and Kishore, 2002) to detect whether a given utterance is neutral or emotional speech. The detection of emotional segments or emotion events may help the current approaches in automatic emotion recognition. This approach avoids the interrelations among the lexical content used, language, and emotional state across varying acoustic features. The discrimination capabilities of AANN models are exploited in various areas of speech such as speaker identification,</text>
        <edit type="repetitiveness;conciseness" crr="verification, and recognition" comments="repetitiveness and conciseness - no need to repeat speaker three times">speaker verification, speaker recognition</edit>
        <edit type="punctuation" crr="; language identification; throat microphone processin; audio clip classification; etc." comments="continuation - comma to semicolon since first list item now has commas">, language identification, throat microphone processin, audio clip classification, etc.</edit>
        <text>(Reddy et al., 2010; Murty and Yegnanarayana, 2006; Yegnanarayana et al., 2001; Mary and Yegnanarayana, 2008; Bajpai and Yegnanarayana, 2008; Shahina and Yegnanarayana, 2007).</text>
        <text>\\ This present work is based on our previous work (Gangamohan et al., 2013) for capturing the deviations of emotional speech from neutral speech. In that paper</text>
        <edit type="style" crr="" comments="style - no need to repeat cite here">(Gangamohan et al., 2013)</edit>
        <text>, it was shown that the excitation source features extracted in the high signal to noise ratio (SNR) regions of the speech signal (around the glottal closure) capture the deviations of emotional speech from neutral speech. This paper presents a framework to characterize the high SNR regions of the speech signal using the knowledge of the speech production mechanism. In (Reddy et al., 2010; Murty and Yegnanarayana, 2006; B. Yegnanarayana and S. R. Mahadeva Prasanna and K. Sreenivasa Rao, 2002), the authors showed the importance of processing the high SNR regions of the speech signal for various applications such as speaker recognition (Reddy et al., 2010; Murty and Yegnanarayana, 2006), speech enhancement (B. Yegnanarayana and S. R. Mahadeva Prasanna and K. Sreenivasa Rao, 2002), emotion analysis (Gangamohan et al., 2013), etc. Hence, in this study, our focus is on the processing of high SNR regions of speech.</text> 
        <text>\\ The remaining part of the paper is organized as follows: Section 2 describes the basis for the present study. Databases used and feature extraction procedures are described in Section 3. In Sections 4 and 5, descriptions of the AANN models for capturing the excitation source and vocal tract system information are given. Emotion detection experiments and discussion on results are given in Section 6. Finally, Section 7 gives a summary and scope for further study.</text>
    </introduction>   
</doc>