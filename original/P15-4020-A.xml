<?xml version="1.0" encoding="UTF-8"?>
<doc id="P15-4020" editor="A" format="Conf" position="NS" region="N">
    <title>
        <text>Multi-level Translation Quality Prediction with QUEST++</text>
    </title>
    <abstract>
        <text>This paper presents QUEST++, an open source tool for quality estimation which can predict quality for texts at</text>
        <edit type="grammar" crr="the" comments="grammar - missing article."></edit>
        <text>word, sentence</text>
        <edit type="punctuation" crr="," comments="adding serial comma, though this can be style specific"></edit>
        <text>and document</text>
        <edit type="grammar" crr="levels" comments="grammar - need plural since we have three levels">level</edit>
        <text>. It also provides pipelined processing, whereby predictions made at a lower level (e.g., for words) can be used as input to build models for predictions at a higher level (e.g., sentences). QUEST++ allows the extraction of a variety of features and provides machine learning algorithms to build and test quality-estimation models. Results on recent datasets show that QUEST++ achieves state-of-the-art performance.</text>
    </abstract>   
    <introduction>
        <text>Quality Estimation (QE) of Machine Translation (MT)</text>
        <edit type="grammar" crr="has" comments="grammar - need the singular here">have</edit>
        <text>become increasingly popular over the last decade. With the goal of providing a prediction on the quality of a machine translated text, QE systems have the potential to make MT more useful in a number of scenarios, for example, improving post-editing efficiency (Specia, 2011), selecting high-quality segments (Soricut and Echihabi, 2010), selecting the best translation (Shah and Specia, 2014), and highlighting words or phrases that need revision (Bach et al., 2011).</text>
        <text>\\ Most recent work focuses on sentence-level QE. This variant is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of sentence translations annotated with quality labels</text>
        <edit type="capitalization;style" crr="(e.g. 1-5 Likert scores)." comments="capitalization and styling - Likert is capitalized as a proper noun and doesn't require italicization">(e.g. 1-5 likert scores).</edit>
        <edit type="clarity" crr="Sentence-level QE has been covered annually in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) since 2012." comments="clarity - moving the adverb closer to what it modifies (covered)">Sentence-level QE has been covered in shared tasks organised by the Workshop on Statistical Machine Translation (WMT) annually since 2012.</edit>
        <text>While standard algorithms can be used to build prediction models, key to this task is the work of feature engineering. Two open source feature extraction toolkits are available for that: ASIYA and QUEST (Specia et al., 2013). The latter has been used as the official baseline for the WMT shared tasks and extended by a number of participants, leading to improved results over the years (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014).</text>
        <text>\\ QE at other textual levels</text>
        <edit type="grammar" crr="has" comments="grammar - need the singular here">have</edit>
        <text> received much less attention. Word-level QE (Blatz et al., 2004; Luong et al., 2014) is seemingly a more challenging task where a quality label is to be produced for each target word. An additional challenge is the acquisition of sizable training sets. Although significant efforts have been made, there is considerable room for improvement. In fact, most WMT13-14 QE shared task submissions were unable to beat a trivial baseline.</text>
        <text>\\ Document-level QE consists</text>
        <edit type="word choice" crr="of" comments="word choice - of is a better preposition here">in</edit>
        <text>predicting a single label for entire documents, be it an absolute score (Scarton and Specia, 2014) or a relative ranking of translations by one or more MT systems (Soricut and Echihabi, 2010). While certain sentences are perfect in isolation, their combination in context may lead to an incoherent document. Conversely, while a sentence can be poor in isolation, when put in context, it may benefit from information in surrounding sentences, leading to a good quality document. Feature engineering is a challenge given the</text>
        <edit type="word choice;clarity" crr="few available" comments="word choice and clarity - I think this more clearly communicates that the issue there aren't many tools available for use.">little availability of</edit>
        <text>tools to extract discourse-wide information. In addition, no datasets with human-created labels are available, and thus, scores produced by automatic metrics have to be used as approximation (Scarton et al., 2015).</text>
        <text>\\ Some applications require fine-grained, word-level information on quality. For example, one may want to highlight words that need fixing. Document-level QE is needed particularly for gisting purposes where post-editing is not an option</text>
        <edit type="grammar" crr=", for example," comments="grammar - merging the sentences since the second isn't a full sentence">. For example,</edit>
        <text>for predictions on translations of product reviews in order to decide whether or not they are understandable by readers. We believe that the limited progress in</text>
        <edit type="hyphenation" crr="word-" comments="hyphenation - need suspended hyphen here since 'level' applies to 'word' as well as 'document'">word</edit>
        <text>and document-level</text>
        <edit type="spelling" crr="QE" comments="spelling - correcting QA to QE">QA</edit>
        <text>research is partially due to lack of a basic framework that one can</text>
        <edit type="grammar" crr="" comments="grammar - 'be build' isn't a valid construction. Alternately, could drop 'one' and use 'can be built'">be</edit>
        <text>build upon and extend.</text>
        <text>\\ QUEST++ is a significantly refactored and expanded version of an existing open source sentence-level toolkit, QUEST. Feature extraction modules for both</text>
        <edit type="hyphenation" crr="word-" comments="hyphenation - need suspended hyphen here since 'level' applies to 'word' as well as 'document'">word</edit>
        <text>and document-level QE were added and the three levels of prediction were unified into a single pipeline, allowing for interactions between</text>
        <edit type="hyphenation" crr=" word-, sentence-," comments="hyphenation - need suspended hyphen here since 'level' applies to 'word' as well as 'document'">word, sentence,</edit>
        <text>and document-level QE. For example, word-level predictions can be used as features for sentence-level QE. Finally, sequence-labelling learning algorithms for word-level QE were added. QUEST++ can be easily extended with new features at any textual level. The architecture of the system is described in Section 2. Its main component, the feature extractor, is presented in Section 3. Section 4 presents experiments using the framework with various datasets.</text>
    </introduction>   
</doc>