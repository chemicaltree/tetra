<?xml version="1.0" encoding="UTF-8"?>
<doc id="P15-2115" editor="B" format="Conf" position="S" region="N">
    <title>
        <text>Machine Comprehension with Syntax, Frames, and Semantics</text>
    </title>
    <abstract>
        <text>We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrently -published results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension.</text>
    </abstract>   
    <introduction>
        <text>Recent question answering (QA) systems (Ferrucci et al., 2010; Berant et al., 2013; Bordes et al., 2014) have focused on open-domain factoid questions, relying on knowledge bases like Freebase (Bollacker et al., 2008) or large corpora of unstructured text. While clearly useful, this type of QA may not be the best way to evaluate natural language understanding capability. Due to the redundancy of facts expressed on the web, many questions are answerable</text>
        <edit type="word choice" crr="using" comments="">with</edit>
        <text>shallow</text>
        <edit type="word order" crr="information extraction techniques" comments="">techniques from information extraction</edit>
        <text>(Yao et al., 2014).</text>
        <edit type="word choice" crr="\n\n Recent work" comments="">\n\n There is also recent work</edit>
        <edit type="word choice" crr="investigated" comments="">on</edit>
        <text>QA based on synthetic text describing events in adventure games (Weston et al., 2015; Sukhbaatar et al., 2015). Synthetic text provides a cleanroom environment for evaluating QA systems</text>
        <edit type="punctuation" crr="" comments="">,</edit>
        <text>and has spurred development of powerful neural architectures for complex reasoning. However, the formulaic semantics underlying these synthetic texts allows for the construction of perfect rule-based question answering systems, and may not reflect the patterns of natural linguistic expression.</text>
        <text>\n\n In this paper, we focus on machine comprehension, which is QA in which the answer is contained within a provided passage. Several comprehension tasks have been developed, including Remedia (Hirschman et al., 1999), CBC4kids (Breck et al., 2001), and the QA4MRE textual question answering tasks in the CLEF evaluations (Peñas et al., 2011; Peñas et al., 2013; Clark et al., 2012; Bhaskar et al., 2012).</text>
        <text>\n\n We consider the Machine Comprehension of Text dataset (MCTest; Richardson et al., 2013), a set of human-authored fictional stories with associated multiple-choice questions. Knowledge bases and web corpora are not useful for this task, and answers are typically expressed just once in each story. While simple baselines presented by Richardson et al. answer over 60% of questions correctly, many of the remaining questions require deeper analysis.</text>
        <text>\n\n In this paper, we explore the use of dependency syntax, frame semantics, word embeddings, and coreference for improving performance on</text>
        <edit type="grammar" crr="the" comments=""></edit>
        <text>MCTest. Syntax, frame semantics, and coreference are essential for understanding who did what to whom. Word embeddings address variation in word choice between the stories and questions. Our added features achieve the best results we are aware of on this dataset, outperforming</text>
        <edit type="punctuation" crr="concurrently published" comments="punctuation; adverbs are not normally linked with adjectives to form compound adjectives">concurrently-published</edit>
        <text>results (Narasimhan and Barzilay, 2015; Sachan et al., 2015).</text>
    </introduction>   
</doc>