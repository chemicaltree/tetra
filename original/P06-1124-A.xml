<?xml version="1.0" encoding="UTF-8"?>
<doc id="P06-1124" editor="A" format="Conf" position="NS" region="NN">
    <title>
        <text>A Hierarchical Bayesian Language Model based on Pitman-Yor Processes</text>
    </title>
    <abstract>
        <text>We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney.</text>
    </abstract>   
    <introduction>
        <text>Probabilistic language models are used extensively in a variety of linguistic applications, including speech recognition, handwriting recognition, optical character recognition, and machine translation. Most language models fall into the class of n-gram models, which approximate the distribution over sentences using the conditional distribution of each word given a context consisting of only the previous n - 1 words, [MATH] with n = 3 (trigram models) being typical. Even for such a modest value of n</text>
        <edit type="punctuation" crr="," comments="punctuation - need comma after intro phrase"></edit>
        <text>the number of parameters is still tremendous due to the large vocabulary size. As a result</text>
        <edit type="punctuation" crr="," comments="punctuation - need comma after intro phrase"></edit>
        <text>direct maximum-likelihood parameter fitting severely overfits to the training data, and smoothing methods are</text>
        <edit type="spelling" crr="indispensable" comments="spelling - indispensible is a valid alternate spelling per MW Unabridged and OED, but is rare. Going with the more common variant.">indispensible</edit>
        <text>for proper training of n-gram models.</text>
        <text>\\ A large number of smoothing methods have been proposed in the literature (see (Chen and Goodman, 1998; Goodman, 2001; Rosenfeld, 2000) for good overviews). Most methods take a rather ad hoc approach, where n-gram probabilities for various values of n are combined</text>
        <edit type="conciseness" crr="" comments="conciseness - together is redundant with combined">together</edit>
        <edit type="punctuation" crr="" comments="punctuation - this seems better without the comma as the using phrase appears to be restrictive">,</edit>
        <text>using either interpolation or back-off schemes. Though some of these methods are intuitively appealing, the main justification has always been empirical—better perplexities or error rates on test data. Though</text>
        <edit type="style;word order" crr=" this arguably" comments="moving the adverb closer to the verb improves flow">arguably this</edit>
        <text>should be the only real justification, it only answers the question of whether a method performs better, not how</text>
        <edit type="word choice" crr="or" comments="word choice - 'nor' goes with 'neither' - otherwise it's better to use 'or'">nor</edit>
        <text>why it performs better. This is unavoidable given that most of these methods are not based on internally coherent Bayesian probabilistic models, which have explicitly declared prior assumptions and whose merits can be argued in terms of how closely these fit in with the known properties of natural languages. Bayesian probabilistic models also have additional advantages—it is relatively straightforward to improve these models by incorporating additional knowledge sources and to include them in larger models in a principled manner. Unfortunately, the performance of previously proposed Bayesian language models had been dismal compared to other smoothing methods (Nadas, 1984; MacKay and Peto, 1994).</text>
        <text>\\ In this paper, we propose a novel language model based on a hierarchical Bayesian model (Gelman et al., 1995) where each hidden variable is distributed according to a Pitman-Yor process, a nonparametric generalization of the Dirichlet distribution that is widely studied in the statistics and probability theory communities (Pitman and Yor, 1997; Ishwaran and James, 2001; Pitman, 2002). Our model is a direct generalization of the hierarchical Dirichlet language model of (MacKay and Peto, 1994). Inference in our model is, however, not as straightforward, and we propose an efficient Markov chain Monte Carlo sampling scheme.</text>
        <text>\\ Pitman-Yor processes produce power-law distributions that more closely resemble those seen in natural languages, and it has been argued that as a result they are more suited to applications in natural language processing (Goldwater et al., 2006). We show experimentally that our hierarchical Pitman-Yor language model does indeed produce results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney, two of the currently best performing smoothing methods (Chen and Goodman, 1998). In fact we show a stronger result—that interpolated Kneser-Ney can be interpreted as a particular approximate inference scheme in the hierarchical Pitman-Yor language model. Our interpretation is more useful than past interpretations involving marginal constraints (Kneser and Ney, 1995; Chen and Goodman, 1998) or maximum-entropy models (Goodman, 2004) as it can recover the exact formulation of interpolated Kneser-Ney, and actually produces superior results. (Goldwater et al., 2006) has independently noted the correspondence between the hierarchical Pitman-Yor language model and interpolated Kneser-Ney and conjectured improved performance in the hierarchical Pitman-Yor language model, which we verify here.</text>
        <text>\\ Thus, the contributions of this paper are threefold: in proposing a</text>
        <edit type="spelling" crr="language" comments="">langauge</edit>
        <text>model with excellent performance and the accompanying advantages of Bayesian probabilistic models, in proposing a novel and efficient inference scheme for the model, and in establishing the direct correspondence between interpolated Kneser-Ney and the Bayesian approach.</text>
        <text>\\ We describe the Pitman-Yor process in Section 2</text>
        <edit type="punctuation" crr="" comments="punctuation - no comma needed before dependent clause">,</edit>
        <text>and propose the hierarchical Pitman-Yor language model in Section 3. In Sections 4 and 5</text>
        <edit type="punctuation" crr="," comments="punctuation - comma after intro phrase"></edit>
        <text>we give a</text>
        <edit type="hyphenation" crr="high-level description" comments="hyphenation - need the hyphen in this phrasal adjective">high level description</edit>
        <text>of our sampling-based inference scheme, leaving the details to a technical report (Teh, 2006). We also show how interpolated Kneser-Ney can be interpreted as approximate inference in the model. We show experimental comparisons to interpolated and modified Kneser-Ney and the hierarchical Dirichlet language model in Section 6 and conclude in Section 7.</text>
    </introduction>   
</doc>