<?xml version="1.0" encoding="UTF-8"?>
<doc id="P16-1061" editor="A" format="Conf" position="S" region="N">
    <title>
        <text>Improving Coreference Resolution by Learning Entity-Level Distributed Representations</text>
    </title>
    <abstract>
        <text>A long-standing challenge in coreference resolution has been the incorporation of entity-level information features defined over clusters of mentions instead of mention pairs. We present a neural network-based coreference system that produces high-dimensional vector representations for pairs of coreference clusters. Using these representations, our system learns</text>
        <edit type="clarity;readability" crr="when it is desirable to combine clusters" comments="clarity/readability - original wording could cause momentary confusion over whether we are talking about when the network learns vs learning when it should combine the clusters. I believe this better captures the intended meaning.">when combining clusters is desirable</edit>
        <text> We train the system with a learning-to-search algorithm that teaches it which local decisions (cluster merges) will lead to a high-scoring final coreference partition. The system substantially outperforms the current state-of-the-art on the English and Chinese portions of the CoNLL 2012 Shared Task dataset despite using few hand-engineered features.</text>                               
    </abstract>   
    <introduction>
        <text>Coreference resolution, the task of identifying which mentions in a text refer to the same real-world entity, is fundamentally a clustering problem. However, many recent state-of-the-art coreference systems operate solely by linking pairs of mentions together (Durrett and Klein, 2013; Martschat and Strube, 2015; Wiseman et al., 2015).</text>
        <text>\n\n An alternative approach is to use agglomerative clustering, treating each mention as a singleton cluster at the outset and then repeatedly merging clusters of mentions deemed to be referring to the same entity. Such systems can take advantage of entity-level information, i.e., features between clusters of mentions instead of between just two mentions. As an example</text>
        <edit type="word choice" crr="of" comments="word choice, wrong preposition">for</edit>
        <text>why this is useful, it is clear that the clusters {Bill Clinton} and {Clinton, she} are not referring to the same entity, but it is ambiguous whether the pair of mentions Bill Clinton and Clinton are coreferent.</text>
        <text>\n\n Previous work has incorporated entity-level information through features that capture hard constraints like having gender or number agreement between clusters (Raghunathan et al., 2010; Durrett et al., 2013). In this work, we instead train a deep neural network to build distributed representations of pairs of coreference clusters. This captures entity-level information with a large number of learned, continuous features instead of a small number of hand-crafted categorical ones.</text>
        <text>\n\n Using the cluster-pair representations, our network learns</text>
        <edit type="clarity;readability" crr="when it is desirable to combine two coreference clusters" comments="clarity/readability - original form could cause momentary confusion over whether we're talking about whether the network learns while it's combining or whens when it should combine. I believe this captures the intended meaning better">when combining two coreference clusters is desirable</edit>
        <text>At test time</text>
        <edit type="punctuation" crr="," comments="punctuation, missing comma on introductory phrase"></edit>
        <text>it builds up coreference clusters incrementally, starting with each mention in its own cluster and then merging a pair of clusters</text>
        <edit type="grammar" crr="at" comments="grammar, missing word"></edit>
        <text>each step. It makes these decisions with a novel easy-first cluster-ranking procedure that combines the strengths of cluster-ranking (Rahman and Ng, 2011) and easy-first (Stoyanov and Eisner, 2012) coreference algorithms.</text>
        <text>\n\n Training incremental coreference systems is challenging because the coreference decisions facing a model depend on previous decisions it has already made. We address this by using a learning-to-search algorithm inspired by SEARN (Daum√© III et al., 2009) to train our neural network. This approach allows the model to learn which action (a cluster merge) available from the current state (a partially completed coreference clustering) will eventually lead to a high-scoring coreference partition.</text>
        <text>\n\n Our system uses little manual feature engineering, which means it is easily extended to multiple languages. We evaluate our system on the English and Chinese portions of the CoNLL 2012 Shared Task dataset. The cluster-ranking model significantly outperforms a mention-ranking model that does not use entity-level information. We also show that using an easy-first strategy improves the performance of the cluster-ranking model. Our final system achieves CoNLL F1 scores of 65.29 for English and 63.66 for Chinese, substantially outperforming other state-of-the-art systems.</text>                                                       
    </introduction>   
</doc>