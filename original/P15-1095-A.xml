<?xml version="1.0" encoding="UTF-8"?>
<doc id="P15-1095" editor="A" format="Conf" position="S" region="N">
    <title>
        <text>Robust Subgraph Generation Improves Abstract Meaning Representation Parsing</text>
    </title>
    <abstract>
        <text>The Abstract Meaning Representation (AMR) is a representation for open-domain rich semantics, with potential use in fields like event extraction and machine translation. Node generation, typically done using a simple dictionary lookup, is currently an important limiting factor in AMR parsing. We propose a small set of actions that derive AMR subgraphs</text>
        <edit type="word choice" crr="using" comments="word choice - I think 'using' is a better preposition here, though I'd really need to see the whole paper to confirm. It's possible 'of' would work better.">by</edit>
        <text>transformations on spans of text, which allows for more robust learning</text>
        <edit type="word choice" crr="at" comments="word choice - I think 'at' is a better preposition, though I would need to review the whole paper to be sure.">of</edit>
        <text> this stage. Our set of construction actions generalize better than the previous approach, and can be learned with a simple classifier. We improve on the previous state-of-the-art result for AMR parsing, boosting end-to-end performance by 3 F1 on both the LDC2013E117 and LDC2014T12 datasets.</text>
    </abstract>   
    <introduction>
        <text>The Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a rich, graph-based language for expressing semantics over a broad domain. The formalism is backed by a large data-labeling effort, and it holds promise for enabling a new breed of natural language applications ranging from semantically aware MT to rich broad-domain QA over text-based knowledge bases. Figure 1 shows an example AMR for “he gleefully ran to his dog Rover,” and we give a brief introduction to AMR in Section 2. This paper focuses on AMR parsing, the task of mapping a natural language sentence into an AMR graph.</text>
        <text>\\ We follow previous work (Flanigan et al., 2014) in dividing AMR parsing into two steps. The first step is concept identification, which generates AMR nodes from text, and which we'll refer to as NER++ (Section 3.1). The second step is relation identification, which adds arcs to link these nodes into a fully connected AMR graph,</text>
        <edit type="clarity" crr="and" comments="clarity - added to better tie the abbreviation to the step rather than the graph. I think that's the intended meaning, but would need the entire paper to confirm."></edit>
        <text>which we'll call SRL++ (Section 3.2).</text>
        <text>We observe that SRL++ is not the hard part of AMR parsing; rather, much of the difficulty in AMR is generating high- accuracy concept subgraphs from the NER++ component. For example, when the existing AMR parser JAMR (Flanigan et al., 2014) is given a gold NER++ output, and must only perform SRL++ over given subgraphs, it scores 80 F1 - nearly the inter-annotator agreement of 83 F1, and far higher than its</text>
        <edit type="hyphenation" crr=" end-to-end" comments="hyphenation - compound adjective">end to end</edit>
        <text>accuracy of 59 F1.</text>
        <text>\\ SRL++ within AMR is relatively easy given a perfect NER++ output, because so much pressure is put on the output of NER++ to carry meaningful information. For example,</text>
        <edit type="word choice" crr="there is" comments="word choice - I would typically avoid contractions in papers as being too casual.">there's</edit>
        <text>a strong typecheck feature for the existence and type of any arc</text>
        <edit type="tone" crr="that only requires" comments="tone - slightly more formal">just by</edit>
        <text>looking at its end-points, and syntactic dependency features</text>
        <edit type="clarity" crr="that" comments="clarity - adding 'that' to make it clear that we're describing features that are part of NER++ versus a more generic statement."></edit>
        <text>are very informative for removing any remaining ambiguity. If a system is considering how to link the node run-01 in Figure 1, the verb-sense frame for “run-01” leaves very little uncertainty for what we could assign as an ARG0 arc. It must be a noun, which leaves either he or dog, and this is easily decided in favor of he by looking for an nsubj arc in the dependency parse.</text>
        <text>\\ The primary contribution of this paper is a novel approach to the NER++ task, illustrated in Figure 2. We notice that the subgraphs aligned to lexical items can often be generated from a small set of generative actions which generalize across tokens. For example, most verbs generate an AMR node corresponding to the verb sense of the appropriate PropBank frame – e.g., run generates run-01 in Figure 1. This allows us to frame the NER++ task as the task of classifying one of a small number of actions for each token, rather than choosing a specific AMR subgraph for every token in the sentence.</text>
        <text>\\ Our approach to the end-to-end AMR parsing task is therefore as follows: we define an action space for generating AMR concepts, and create a classifier for classifying lexical items into one of these actions (Section 4). This classifier is trained from automatically generated alignments between the gold AMR trees and their associated sentences (Section 5), using an objective which favors alignment mistakes which are least harmful to the NER++ component. Finally, the concept subgraphs are combined into a coherent AMR parse using the maximum spanning connected subgraph algorithm of Flanigan et al. (2014).</text>
        <text>\\  We show that our approach provides a large boost to recall over previous approaches</text>
        <edit type="punctuation" crr="" comments="punctuation - removing comma from list of two items.">,</edit>
        <text>and that</text>
        <edit type="hyphenation" crr=" end-to-end" comments="hyphenation - compound adjective">end to end</edit>
        <text>performance is improved from 59 to 62 smatch (an F1 measure of correct AMR arcs; see Cai and Knight (2013)) when incorporated into the SRL++ parser of Flanigan et al. (2014). When evaluating the performance of our action classifier in isolation, we obtain an action classification accuracy of 84.1%.</text>
    </introduction>   
</doc>