<?xml version="1.0" encoding="UTF-8"?>
<doc id="W07-0736" editor="B" format="WS" position="S" region="N">
    <title>
        <text>Sentence Level Machine Translation Evaluation as a Ranking Problem: one step aside from BLEU</text>
    </title>
    <abstract>
        <text>The paper proposes formulating MT evaluation as a ranking problem, as</text>
        <edit type="clarity" crr="it" comments=""></edit>
        <text>is often</text>
        <edit type="style" crr="treated" comments="">done</edit>
        <edit type="word order" crr="in the assessment practices of humans." comments="">in the practice of assessment by human.</edit>
        <text>Under the ranking scenario, the study also investigates the relative utility of several features. The results show greater correlation with human assessment at the sentence level, even when using an n-gram match score as a baseline feature.</text>
        <edit type="grammar" crr="The features contributing the most to the rank order correlation between automatic ranking and human assessment were" comments="plural rather than singular; two different features are addressed in the sentence.">The feature contributing the most to the rank order correlation between automatic ranking and human assessment was</edit>
        <edit type="clarity" crr="the use of" comments=""></edit>
        <text>the dependency structure relation</text>
        <edit type="word choice" crr="as opposed to the" comments="">rather than</edit>
        <text>BLEU score and</text>
        <edit type="clarity" crr="the" comments="added for clarity"></edit>
        <text>reference language model feature.</text>
    </abstract>   
    <introduction>
        <text>In recent decades,</text>
        <edit type="word choice;clarity" crr="with the growing body of research" comments="">alongside the growing research</edit>
        <text>on Machine Translation (MT), automatic MT evaluation has become a critical problem for MT system developers</text> 
        <edit type="conciseness" crr="" comments="extraneous">, who are</edit>
        <text>interested in</text>
        <edit type="word order" crr="development cycles with quick turnarounds." comments="">quick turnaround development cycles.</edit>
        <text>The state-of-the-art automatic MT evaluation is an n-gram based metric represented by BLEU (Papineni et al., 2001) and its variants.</text>
        <edit type="conciseness" crr="Since" comments="extraneous">Ever since</edit>
        <text>its creation, the BLEU score has been</text>
        <edit type="clarity" crr="used as a gauge of" comments="added for clarity">the gauge of</edit>
        <edit type="capitalization" crr="machine translation" comments="capital not needed">Machine Translation</edit>
        <text>system evaluation. Nevertheless, the research community has been largely aware of the deficiency of the BLEU metric. BLEU captures only a single dimension of the vitality of natural languages: a candidate translation</text>
        <edit type="style" crr="is" comments="">gets</edit>
        <text>acknowledged only if it uses exactly the same lexicon as the reference translation. Natural languages, however, are characterized by</text>
        <edit type="conciseness" crr="" comments="extraneous">thier</edit>
        <text>extremely rich mechanisms for</text>
        <edit type="word choice" crr="reproducing statements" comments="">reproduction</edit>
        <text>via a large number of syntactic, lexical and semantic rewriting rules. Although BLEU has been shown to correlate positively with human assessments at the document level (Papineni et al., 2001), efforts to improve state-of-the-art MT require that human assessment be approximated at sentence level as well. Researchers report the BLEU score at the document level in order to combat the sparseness of n-grams in BLEU scoring. But, ultimately, document-level MT evaluation</text>
        <edit type="word choice;conciseness" crr="must" comments="word choice; used for brevity">has to</edit>
        <text>be pinned down to the granularity of the sentence. Unfortunately, the correlation between human assessment and BLEU score at the sentence level is extremely low (Liu et al., 2005, 2006). While acknowledging the appealing simplicity of BLEU as a way to access one perspective of an MT candidate translation's quality, we observe the following facts of n-gram-based MT metrics. First, they may not reflect the mechanism</text>
        <edit type="word choice" crr="used by human beings to" comments="">of how human beings</edit>
        <text>evaluate sentence translation quality. More specifically, optimizing BLEU does not guarantee optimization of</text>
        <edit type="clarity" crr="the type of" comments=""></edit>
        <text>sentence quality approved by human assessors. Therefore, BLEU is likely to have a low correlation with human assessment at the sentence level for most candidate translations. Second, it is conceivable that human beings are more reliable at ranking the quality of multiple candidate translations than assigning a numeric value to index the quality of the candidate translation, even with significant deliberation. Consequently, a more intuitive approach</text>
        <edit type="grammar" crr="to" comments="">for</edit>
        <text>automatic MT evaluation is to replicate the quality ranking ability of human assessors. Thirdly, the BLEU score is elusive and hard to interpret; for example, what can be concluded</text>
        <edit type="clarity;word choice" crr="in terms of" comments="">for</edit>
        <text>a candidate translation's quality if the BLEU score is 0.0168, particularly when we are aware that even a human translation can receive an embarrassingly low BLEU score? In light of the discussion</text>
        <edit type="redundancy" crr="" comments="">above</edit>
        <text>, we propose an alternative scenario for MT evaluation where, instead of assigning a numeric score to a candidate translation under evaluation, we predict its rank with regard to its peer candidate translations. This formulation of the MT evaluation task fills the gap between an automatic scoring function and human MT evaluation practice. The results</text>
        <edit type="grammar" crr="of" comments="">from</edit>
        <text>the current study will not only interest MT system evaluation moderators</text>
        <edit type="readability" crr=", it" comments="">but</edit>
        <text>will also inform the research community about which features are useful in improving the correlation between human</text>
        <edit type="readability" crr="" comments="omitted for concision; redundant word">rankings</edit>
        <text>and automatic rankings.</text>
    </introduction>   
</doc>