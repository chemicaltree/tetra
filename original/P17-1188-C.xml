<?xml version="1.0" encoding="UTF-8"?>
<doc id="P17-1188" editor="C" format="Conf" position="S" region="N">
    <title>
        <text>Learning Character-level Compositionality with Visual Featuresn</text>
    </title>
    <abstract>
        <text>Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems, compositionality has an effect</text>
        <edit type="punctuation" crr="," comments="use comma to separate clause"></edit>
        <text>even on the character-level: the meaning of a character is derived</text>
        <edit type="grammar" crr="from" comments="preposition 'from' agrees with 'derived'">by</edit>
        <text>the sum of its parts. In this paper, we model this effect by creating embeddings for characters, based on their visual characteristics</text>
        <edit type="readability" crr=". We create an image for the character and run" comments="End long and complex sentence">, creating an image for the character and running</edit>
        <text>it through a convolutional neural network</text>
        <edit type="style" crr="in order to" comments="'to' is correct, but the proposed alternative is more specific in academic style when denoting intent">to</edit>
        <text>produce a visual character embedding. Experiments on a text classification task demonstrate that such a model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content, resulting in embeddings that are coherent in visual space.</text>
    </abstract>   
    <introduction>
        <text>Compositionality—the fact that the meaning of a complex expression is determined by its structure and the meanings of its constituents—is a hallmark of every natural language (Frege and Austin, 1980; Szabó, 2010). Recently, neural models have provided a powerful tool for learning how to</text>
        <edit type="clarity" crr="weave" comments="eplace to clarify meaning">compose</edit>
        <text>words together into a meaning representation of whole sentences for many downstream tasks. This is</text>
        <edit type="style" crr="achieved" comments="">done</edit>
        <text>using models of various levels of sophistication, from simpler bag-of-words (Iyyer et al., 2015) and linear recurrent neural network (RNN) models (Sutskever et al., 2014; Kiros et al., 2015) to more sophisticated models</text>
        <edit type="grammar" crr="use" comments="">using</edit>
        <text>tree-structured (Socher et al., 2013) or convolutional networks (Kalchbrenner et al., 2014).</text>
        <text>\\ In fact, a growing body of evidence shows that it is essential to look below the</text>
        <edit type="hyphenation" crr="word level" comments="not required, since it is not qualifying a noun that follows">word-level</edit>
        <text>and consider compositionality within words themselves. For example, several works have proposed models that represent words by composing together the characters into a representation of the word itself (Ling et al., 2015; Zhang et al., 2015; Dhingra et al., 2016). Additionally, for languages with productive word formation (such as agglutination and compounding), models</text>
        <edit type="clarity" crr="that calculate" comments="">calculating</edit>
        <text>morphology-sensitive word representations</text>
        <edit type="clarity" crr="found to be" comments="restructure sentence to clarify">have been found</edit>
        <text>effective (Luong et al., 2013; Botha and Blunsom, 2014). These models help to learn more robust representations for rare words by exploiting morphological patterns, as opposed to models that operate purely on the lexical level as</text>
        <edit type="grammar" crr="" comments="">the</edit>
        <text>atomic units.</text>
        <text>\\ For many languages, compositionality stops at the character level: characters are atomic units of meaning or pronunciation in</text>
        <edit type="word usage;grammar" crr="a" comments="indefinite article more appropriate here as you're writing about 'in general'">the</edit>
        <text>language, and no further decomposition can be</text>
        <edit type= crr="carried out." comments="">done.</edit>
        <text>However, for other languages, character-level compositionality, where a character's meaning or pronunciation can be derived from the sum of its parts, is very much a reality. Perhaps the most compelling example of compositionality of sub-character units can be found in logographic writing systems such as the Han and Kanji characters used in Chinese and Japanese respectively. As shown on the left side of Fig. 1, each part of a Chinese character (called a “radical”) potentially contributes to the meaning (i.e., Fig. 1(a)) or pronunciation (i.e., Fig. 1(b)) of the overall character. This is similar to how English characters combine into the meaning or pronunciation of an English word. Even in languages with phonemic orthographies, where each character corresponds to a pronunciation instead of a meaning, there are cases where composition occurs. Fig. 1(c) and (d) show the examples of Korean and German, respectively, where morphological inflection can cause single characters to make changes where some but not</text>
        <edit type="word usage" crr="all the" comments="">all of the</edit>
        <text>component parts are shared.</text>
        <text>\\ In this paper, we investigate the feasibility of modeling the compositionality of characters in a</text>
        <edit type="word usage" crr="similar manner to humans:" comments="">way similar to how humans do:</edit>
        <text>by visually observing the character and using the features of its shape to learn a representation encoding its meaning. Our method is relatively simple and</text>
        <edit type="word usage" crr="generally applicable in" comments="">generalizable to</edit>
        <text>a wide variety of languages: we first transform each character from its Unicode representation to a rendering of its shape as an image</text>
        <edit type="readability" crr=". We then" comments="end potentially long sentence / readability">, then</edit>
        <text>calculate a representation of the image using Convolutional Neural Networks (CNNs) (Cun et al., 1990). These features then serve as inputs to a downstream processing task and trained in an end-to-end manner, which first calculates a loss function, then</text>
        <edit type="redundancy" crr="" comments="duplication of 'back'">back-</edit>
        <text>propagates the loss back to the CNN.</text>
        <text>\\ As demonstrated by our motivating examples in Fig. 1, in logographic languages character-level semantic or phonetic similarity is often indicated by visual cues; we conjecture that CNNs can appropriately model these visual patterns. Consequently, characters with similar visual appearances will be biased</text>
        <edit type="clarity" crr="toward" comments="">to</edit>
        <edit type="word usage" crr="" comments="">have</edit>
        <text>similar embeddings</text>
        <edit type="word usage;clarity" crr="which allow" comments="Word usage / clarity / insert to specify">, allowing</edit>
        <text>our model to handle rare characters effectively, just as character-level models have been effective for rare words.</text>
        <text>\\ To evaluate our model's ability to learn representations, particularly for rare characters, we perform experiments on a downstream task of classifying Wikipedia titles for three Asian languages: Chinese, Japanese, and Korean. We</text>
        <edit type="style" crr="demonstrate" comments="'show' is correct, but the proposed alternative is better for academic register">show</edit>
        <text>that our proposed framework outperforms a baseline model that uses standard character embeddings for instances containing rare characters. A qualitative analysis of the characteristics of the learned embeddings of our model demonstrates that visually similar characters share similar embeddings. We also demonstrate that the learned representations are particularly effective under low-resource scenarios and complementary with standard character embeddings</text>
        <edit type="readability" crr=". Combining" comments="end potentially long sentence">; combining</edit>
        <text>the two representations through three different fusion methods (Snoek et al., 2005; Karpathy et al., 2014) leads to consistent improvements over the strongest baseline without visual features.</text>
    </introduction>   
</doc>