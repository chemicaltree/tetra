<?xml version="1.0" encoding="UTF-8"?>
<doc id="W18-3410" editor="A" format="WS" position="NS" region="N">
    <title>
        <text>Low-rank passthrough neural networks</text>
    </title>
    <abstract>
        <text>Various common deep learning architectures, such as LSTMs, GRUs, Resnets, and Highway Networks, employ</text>
        <edit type="hyphenation" crr="state-passthrough" comments="hyphenation - compound adjective">state passthrough</edit>
        <text>connections that support training with high feed-forward depth or recurrence over many time steps. These “Passthrough Networks” architectures also enable the decoupling of the network-state size from the number of parameters of the network, a possibility</text>
        <edit type="grammar" crr="that" comments=""></edit>
        <text>has been studied by Sak et al. (2014) with their low-rank parametrization of the LSTM. In this work, we extend this line of research, proposing effective, low-rank and low-rank plus diagonal-matrix parametrizations for Passthrough Networks which exploit this decoupling property, reducing the data complexity and memory requirements of the network while preserving its memory capacity. This is particularly beneficial in low-resource settings as it supports expressive models with a compact parametrization less susceptible to overfitting. We present competitive experimental results on several tasks, including language modeling and a near</text>
        <edit type="hyphenation" crr="state-of-the-art" comments="hyphenation - this is hyphenated as adjective">state of the art</edit>
        <text>result on sequential randomly -permuted MNIST classification, a hard task on natural data.</text>
    </abstract>   
    <introduction>
        <text>Deep neural networks can perform non trivial computations by repeated the application of parametric non-linear transformation layers to vectorial data. This staging of many computation steps can be done over a time dimension for tasks involving sequential inputs or outputs of varying length, yielding a recurrent neural network, or over an intrinsic circuit-depth dimension, yielding a deep feed-forward neural network, or both. Training these deep models is complicated by the exploding and vanishing gradient problems (Hochreiter, 1991; Bengio et al., 1994).</text>
        <text>\\ Various network architectures have been proposed to ameliorate the vanishing gradient problem in the recurrent setting, such as the LSTM (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005), the GRU (Cho et al., 2014), the RHN (Zilly et al., 2016), etc. Similar methods have also been applied in the feed-forward setting with architectures such as Highway Networks (Srivastava et al., 2015), Deep Residual Networks (He et al., 2015), and so on. All these architectures are based on a single structural principle which, in this work, we will refer to as the state passthrough. We will thus refer to these architectures as Passthrough Networks.</text>
        <text>\\ In many settings, especially low-resource natural language processing tasks, the main difficulty in training neural networks is the trade-off between the network representation power and its training data complexity, which is related to the number of trainable parameters.</text>
        <text>\\ On one hand, the number of parameters can be thought</text>
        <edit type="grammar" crr="of" comments=""></edit>
        <text>as the number of tunable “knobs” that need to be set to represent a function</text>
        <edit type="punctuation" crr=";" comments="punctuation - semicolon needed between independent clauses">,</edit>
        <text>on the other hand, it also constrains the size of the partial results that are propagated inside the network. In typical fully connected networks, a layer acting on a n-dimensional state vector has O(n2) parameters stored in one or more matrices, but there can be many functions of practical interest that are simple enough to be represented by a relatively small number of bits while still requiring some sizable amount of memory in order to be computed. Therefore, representing these functions on a fully connected neural network can be wasteful in terms of the number of parameters. The full parameterization implies that, at each step, all the information in each state component can affect all the information in any state component at the next step. Classical physical systems, however, consist of spatially separated parts with primarily local interactions</text>
        <edit type="punctuation" crr=";" comments="punctuation - semicolon needed between independent clauses">,</edit>
        <text>long-distance interactions are possible, but they tend to be limited by propagation delays, bandwidth, and noise. Therefore, it may be beneficial to bias our model class towards models that tend to adhere to these physical constraints by using a parametrization which reduces the number of parameters required to represent them. This can be accomplished by imposing some constraints on the n * n matrices that parametrize the state transitions. One way of doing this is to impose a convolutional structure on these matrices (LeCun et al., 2004; Krizhevsky et al., 2012), which corresponds to strict locality and periodicity constraints as in a cellular automaton. These constraints work well in certain domains such as vision, but may be overly restrictive in other domains.</text>
        <text>\\ The state passthrough allows for a systematic decoupling of the</text>
        <edit type="hyphenation" crr="network-state" comments="hyphenation - needed in compound adjective">network state</edit>
        <text>size from the number of parameters: since by default the state vector passes mostly unaltered through the layers, each layer can be made simple enough to be described only by a small number of parameters without affecting the</text>
        <edit type="conciseness" crr="network's overall memory capacity" comments="conciseness - one word vs 3">overall memory capacity of the network</edit>
        <text>, effectively spreading the computation over the</text>
        <edit type="conciseness" crr="network's depth or time dimension" comments="conciseness - one word vs 3">depth or time dimension of the network</edit>
        <text>, but without making the network “thin”. This has been exploited by some convolutional passthrough architectures (Srivastava et al., 2015; He et al., 2015; Kaiser and Sutskever, 2015)</text>
        <edit type="punctuation" crr="" comments="punctuation - don't need comma in two item list.">,</edit>
        <edit type="word choice" crr="and" comments="word choice - I think we want 'and' here as I assume both of these meet the criteria">or</edit>
        <text>architectures with addressable read-write memory (Graves et al., 2014; Danihelka et al., 2016).</text>
        <text>\\ In this work, we propose simple but effective low-dimensional parametrizations that exploit this decoupling based on low-rank or low-rank plus diagonal-matrix decompositions. Our approach extends the LSTM architecture with a single projection layer proposed by Sak et al. (2014) which has been applied to speech recognition, natural language modeling (Józefowicz et al., 2016), video analysis (Sun et al., 2015), etc. We provide experimental evaluation of our approach on GRU and LSTM architectures on various machine learning tasks, including a near</text>
        <edit type="hyphenation" crr="state-of-the-art" comments="hyphenation - this is hyphenated as adjective">state of the art</edit>
        <text>result for the hard task of sequential randomly-permuted MNIST image recognition (Le et al., 2015).</text>
    </introduction>   
</doc>